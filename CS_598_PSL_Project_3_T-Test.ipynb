{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098d1b18",
   "metadata": {},
   "source": [
    "# CS 598 PSL Project 3: approach based on Campuswire post [628](https://campuswire.com/c/G06C55090/feed/628)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c705d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "SEED = 4031\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e54402b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in datasets\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "test_ys = []\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    folder = f\"Data/split_{fold+1}/\"\n",
    "    train_datasets.append(pd.read_csv(folder + \"train.tsv\", sep=\"\\t\"))\n",
    "    test_datasets.append(pd.read_csv(folder + \"test.tsv\", sep=\"\\t\"))\n",
    "    test_ys.append(pd.read_csv(folder + \"test_y.tsv\", sep=\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443c6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stopword list\n",
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"their\", \"they\", \"his\", \\\n",
    "             \"her\", \"she\", \"he\", \"a\", \"an\", \"and\", \"is\", \"was\", \"are\", \"were\", \"him\", \"himself\", \"has\", \"have\", \"it\", \"its\", \\\n",
    "             \"the\", \"us\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590891c",
   "metadata": {},
   "source": [
    "## Construct vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11e6d8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Afraid of the Dark left me with the impression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>This has to be one of the biggest misfires eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This is one of those movies I watched, and won...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>This movie was dreadful. Biblically very inacc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124995</th>\n",
       "      <td>0</td>\n",
       "      <td>I am a student of film, and have been for seve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124996</th>\n",
       "      <td>0</td>\n",
       "      <td>It seems like more consideration has gone into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124997</th>\n",
       "      <td>0</td>\n",
       "      <td>I don't believe they made this film. Completel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124998</th>\n",
       "      <td>0</td>\n",
       "      <td>This 30 minute documentary Buñuel made in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124999</th>\n",
       "      <td>1</td>\n",
       "      <td>I saw this movie as a child and it broke my he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                             review\n",
       "0               1  Naturally in a film who's main themes are of m...\n",
       "1               0  Afraid of the Dark left me with the impression...\n",
       "2               0  This has to be one of the biggest misfires eve...\n",
       "3               0  This is one of those movies I watched, and won...\n",
       "4               0  This movie was dreadful. Biblically very inacc...\n",
       "...           ...                                                ...\n",
       "124995          0  I am a student of film, and have been for seve...\n",
       "124996          0  It seems like more consideration has gone into...\n",
       "124997          0  I don't believe they made this film. Completel...\n",
       "124998          0  This 30 minute documentary Buñuel made in the ...\n",
       "124999          1  I saw this movie as a child and it broke my he...\n",
       "\n",
       "[125000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use all training data to construct vocabulary.\n",
    "\n",
    "all_train = pd.concat(train_datasets, axis=0, ignore_index=True)\n",
    "all_train.drop(columns=[\"id\"], inplace=True)\n",
    "all_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a62e0",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9823e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(reviews):\n",
    "    \n",
    "    \"\"\"\n",
    "    Routine to expand English contractions, like \"isn't\" --> \"is not\".\n",
    "    This is because \"isn't good\" and \"wasn't good\" will both expand to produce the bi-gram \"not good\".\n",
    "    The pooled phrase should have more predictive power than the original two phrases.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary of English contractions. Taken from StackOverflow post, which borrowed it from Wikipedia:\n",
    "    # https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "\n",
    "    contractions = { \n",
    "        \"\\\\bain't\\\\b\": \"am not\",\n",
    "        \"\\\\baren't\\\\b\": \"are not\",\n",
    "        \"\\\\bcan't\\\\b\": \"cannot\",\n",
    "        \"\\\\bcan't've\\\\b\": \"cannot have\",\n",
    "        \"\\\\b'cause\\\\b\": \"because\",\n",
    "        \"\\\\bcould've\\\\b\": \"could have\",\n",
    "        \"\\\\bcouldn't\\\\b\": \"could not\",\n",
    "        \"\\\\bcouldn't've\\\\b\": \"could not have\",\n",
    "        \"\\\\bdidn't\\\\b\": \"did not\",\n",
    "        \"\\\\bdoesn't\\\\b\": \"does not\",\n",
    "        \"\\\\bdon't\\\\b\": \"do not\",\n",
    "        \"\\\\bhadn't\\\\b\": \"had not\",\n",
    "        \"\\\\bhadn't've\\\\b\": \"had not have\",\n",
    "        \"\\\\bhasn't\\\\b\": \"has not\",\n",
    "        \"\\\\bhaven't\\\\b\": \"have not\",\n",
    "        \"\\\\bhe'd\\\\b\": \"he would\",\n",
    "        \"\\\\bhe'd've\\\\b\": \"he would have\",\n",
    "        \"\\\\bhe'll\\\\b\": \"he will\",\n",
    "        \"\\\\bhe'll've\\\\b\": \"he will have\",\n",
    "        \"\\\\bhe's\\\\b\": \"he is\",\n",
    "        \"\\\\bhow'd\\\\b\": \"how did\",\n",
    "        \"\\\\bhow'd'y\\\\b\": \"how do you\",\n",
    "        \"\\\\bhow'll\\\\b\": \"how will\",\n",
    "        \"\\\\bhow's\\\\b\": \"how is\",\n",
    "        \"\\\\bi'd\\\\b\": \"i would\",\n",
    "        \"\\\\bi'd've\\\\b\": \"i would have\",\n",
    "        \"\\\\bi'll\\\\b\": \"i will\",\n",
    "        \"\\\\bi'll've\\\\b\": \"i will have\",\n",
    "        \"\\\\bi'm\\\\b\": \"i am\",\n",
    "        \"\\\\bi've\\\\b\": \"i have\",\n",
    "        \"\\\\bisn't\\\\b\": \"is not\",\n",
    "        \"\\\\bit'd\\\\b\": \"it would\",\n",
    "        \"\\\\bit'd've\\\\b\": \"it would have\",\n",
    "        \"\\\\bit'll\\\\b\": \"it will\",\n",
    "        \"\\\\bit'll've\\\\b\": \"it will have\",\n",
    "        \"\\\\bit's\\\\b\": \"it is\",\n",
    "        \"\\\\blet's\\\\b\": \"let us\",\n",
    "        \"\\\\bma'am\\\\b\": \"madam\",\n",
    "        \"\\\\bmayn't\\\\b\": \"may not\",\n",
    "        \"\\\\bmight've\\\\b\": \"might have\",\n",
    "        \"\\\\bmightn't\\\\b\": \"might not\",\n",
    "        \"\\\\bmightn't've\\\\b\": \"might not have\",\n",
    "        \"\\\\bmust've\\\\b\": \"must have\",\n",
    "        \"\\\\bmustn't\\\\b\": \"must not\",\n",
    "        \"\\\\bmustn't've\\\\b\": \"must not have\",\n",
    "        \"\\\\bneedn't\\\\b\": \"need not\",\n",
    "        \"\\\\bneedn't've\\\\b\": \"need not have\",\n",
    "        \"\\\\bo'clock\\\\b\": \"of the clock\",\n",
    "        \"\\\\boughtn't\\\\b\": \"ought not\",\n",
    "        \"\\\\boughtn't've\\\\b\": \"ought not have\",\n",
    "        \"\\\\bshan't\\\\b\": \"shall not\",\n",
    "        \"\\\\bsha'n't\\\\b\": \"shall not\",\n",
    "        \"\\\\bshan't've\\\\b\": \"shall not have\",\n",
    "        \"\\\\bshe'd\\\\b\": \"she would\",\n",
    "        \"\\\\bshe'd've\\\\b\": \"she would have\",\n",
    "        \"\\\\bshe'll\\\\b\": \"she will\",\n",
    "        \"\\\\bshe'll've\\\\b\": \"she will have\",\n",
    "        \"\\\\bshe's\\\\b\": \"she is\",\n",
    "        \"\\\\bshould've\\\\b\": \"should have\",\n",
    "        \"\\\\bshouldn't\\\\b\": \"should not\",\n",
    "        \"\\\\bshouldn't've\\\\b\": \"should not have\",\n",
    "        \"\\\\bso've\\\\b\": \"so have\",\n",
    "        \"\\\\bso's\\\\b\": \"so is\",\n",
    "        \"\\\\bthat'd\\\\b\": \"that would\",\n",
    "        \"\\\\bthat'd've\\\\b\": \"that would have\",\n",
    "        \"\\\\bthat's\\\\b\": \"that is\",\n",
    "        \"\\\\bthere'd\\\\b\": \"there would\",\n",
    "        \"\\\\bthere'd've\\\\b\": \"there would have\",\n",
    "        \"\\\\bthere's\\\\b\": \"there is\",\n",
    "        \"\\\\bthey'd\\\\b\": \"they would\",\n",
    "        \"\\\\bthey'd've\\\\b\": \"they would have\",\n",
    "        \"\\\\bthey'll\\\\b\": \"they will\",\n",
    "        \"\\\\bthey'll've\\\\b\": \"they will have\",\n",
    "        \"\\\\bthey're\\\\b\": \"they are\",\n",
    "        \"\\\\bthey've\\\\b\": \"they have\",\n",
    "        \"\\\\bto've\\\\b\": \"to have\",\n",
    "        \"\\\\bwasn't\\\\b\": \"was not\",\n",
    "        \"\\\\bwe'd\\\\b\": \"we would\",\n",
    "        \"\\\\bwe'd've\\\\b\": \"we would have\",\n",
    "        \"\\\\bwe'll\\\\b\": \"we will\",\n",
    "        \"\\\\bwe'll've\\\\b\": \"we will have\",\n",
    "        \"\\\\bwe're\\\\b\": \"we are\",\n",
    "        \"\\\\bwe've\\\\b\": \"we have\",\n",
    "        \"\\\\bweren't\\\\b\": \"were not\",\n",
    "        \"\\\\bwhat'll\\\\b\": \"what will\",\n",
    "        \"\\\\bwhat'll've\\\\b\": \"what will have\",\n",
    "        \"\\\\bwhat're\\\\b\": \"what are\",\n",
    "        \"\\\\bwhat's\\\\b\": \"what is\",\n",
    "        \"\\\\bwhat've\\\\b\": \"what have\",\n",
    "        \"\\\\bwhen's\\\\b\": \"when is\",\n",
    "        \"\\\\bwhen've\\\\b\": \"when have\",\n",
    "        \"\\\\bwhere'd\\\\b\": \"where did\",\n",
    "        \"\\\\bwhere's\\\\b\": \"where is\",\n",
    "        \"\\\\bwhere've\\\\b\": \"where have\",\n",
    "        \"\\\\bwho'll\\\\b\": \"who will\",\n",
    "        \"\\\\bwho'll've\\\\b\": \"who will have\",\n",
    "        \"\\\\bwho's\\\\b\": \"who is\",\n",
    "        \"\\\\bwho've\\\\b\": \"who have\",\n",
    "        \"\\\\bwhy's\\\\b\": \"why is\",\n",
    "        \"\\\\bwhy've\\\\b\": \"why have\",\n",
    "        \"\\\\bwill've\\\\b\": \"will have\",\n",
    "        \"\\\\bwon't\\\\b\": \"will not\",\n",
    "        \"\\\\bwon't've\\\\b\": \"will not have\",\n",
    "        \"\\\\bwould've\\\\b\": \"would have\",\n",
    "        \"\\\\bwouldn't\\\\b\": \"would not\",\n",
    "        \"\\\\bwouldn't've\\\\b\": \"would not have\",\n",
    "        \"\\\\by'all\\\\b\": \"you all\",\n",
    "        \"\\\\by'all'd\\\\b\": \"you all would\",\n",
    "        \"\\\\by'all'd've\\\\b\": \"you all would have\",\n",
    "        \"\\\\by'all're\\\\b\": \"you all are\",\n",
    "        \"\\\\by'all've\\\\b\": \"you all have\",\n",
    "        \"\\\\byou'd\\\\b\": \"you would\",\n",
    "        \"\\\\byou'd've\\\\b\": \"you would have\",\n",
    "        \"\\\\byou'll\\\\b\": \"you will\",\n",
    "        \"\\\\byou'll've\\\\b\": \"you will have\",\n",
    "        \"\\\\byou're\\\\b\": \"you are\",\n",
    "        \"\\\\byou've\\\\b\": \"you have\"\n",
    "    }\n",
    "    \n",
    "    # Replace all contractions in all reviews.\n",
    "    for contraction in contractions:\n",
    "        reviews = reviews.str.replace(contraction, contractions[contraction], regex=True)\n",
    "        \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1460b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to preprocess text: strip out HTML, convert to lowercase, and expand English contractions.\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \"\"\"\n",
    "    Routine to preprocess text: strip out HTML, convert to lowercase, and expand English contractions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove HTML tags\n",
    "    reviews = reviews.str.replace('<.*?>', ' ', regex=True)\n",
    "    # Convert to lowercase\n",
    "    reviews = reviews.str.lower()\n",
    "    # Expand English contractions\n",
    "    reviews = expand_contractions(reviews)\n",
    "    \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffa4b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all training text\n",
    "all_train[\"review\"] = preprocess_reviews(all_train[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1edfb8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the reviews\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    #preprocessor=lambda x: x.lower(), # Convert to lowercase\n",
    "    stop_words=stopwords,             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                     # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r\"\\b[\\w+\\|']+\\b\"    # Use word tokenizer, but don't split on apostrophes\n",
    ")\n",
    "\n",
    "dtm_train = vectorizer.fit_transform(all_train[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "747cc604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31701,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the number of ngrams\n",
    "feature_ngrams = vectorizer.get_feature_names_out()\n",
    "feature_ngrams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b19b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output features to file\n",
    "np.savetxt(\"all_train_features.txt\", feature_ngrams, fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2f9d9",
   "metadata": {},
   "source": [
    "### Use t-test to identify strongest 2000 positive and negative terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d579ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try t-test to identify terms that are strongly associated with only positive or only negative reviews.\n",
    "\n",
    "# Separate positive and negative reviews\n",
    "dtm_pos = dtm_train[all_train.sentiment == 1, :]\n",
    "dtm_neg = dtm_train[all_train.sentiment == 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3102a72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62385, 62615)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count positive and negative reviews\n",
    "dtm_pos_count = dtm_pos.shape[0]\n",
    "dtm_neg_count = dtm_neg.shape[0]\n",
    "dtm_pos_count, dtm_neg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da0ae763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables to hold means and variances of positive & negative reviews\n",
    "dtm_pos_means = np.empty(feature_ngrams.shape[0])\n",
    "dtm_pos_vars = np.empty(feature_ngrams.shape[0])\n",
    "\n",
    "dtm_neg_means = np.empty(feature_ngrams.shape[0])\n",
    "dtm_neg_vars = np.empty(feature_ngrams.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2534456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature means and sample variances using dense matrices instead of sparse matrices, one column at a time.\n",
    "# This is because computing variance of sparse matrices is reported to have numerical instability.\n",
    "# Use one column at a time to avoid consuming too much memory.\n",
    "# Approach taken from StackOverflow post:\n",
    "# https://stackoverflow.com/questions/12169611/how-do-i-compute-the-variance-of-a-column-of-a-sparse-matrix-in-scipy\n",
    "\n",
    "for col in range(feature_ngrams.shape[0]):\n",
    "    pos_col_array = dtm_pos[:, col].toarray()\n",
    "    dtm_pos_means[col] = np.mean(pos_col_array)\n",
    "    dtm_pos_vars[col] = np.var(pos_col_array, ddof=1)\n",
    "    \n",
    "    neg_col_array = dtm_neg[:, col].toarray()\n",
    "    dtm_neg_means[col] = np.mean(neg_col_array)\n",
    "    dtm_neg_vars[col] = np.var(neg_col_array, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c65289b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31701,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each term / ngram, compute t-statistic for two independent samples.\n",
    "# Hmmm...they're not independent, but we can't really pool the variance...\n",
    "\n",
    "t_statistics = (dtm_pos_means - dtm_neg_means) / np.sqrt((dtm_pos_vars/dtm_pos_count) + (dtm_neg_vars/dtm_neg_count))\n",
    "t_statistics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bef114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_statistic_df = pd.DataFrame({\"feature\": feature_ngrams.tolist(), \"statistic\": t_statistics.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52a01f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>great</td>\n",
       "      <td>73.982716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8232</th>\n",
       "      <td>excellent</td>\n",
       "      <td>58.862798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31186</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>53.246837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>best</td>\n",
       "      <td>53.155283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17806</th>\n",
       "      <td>of best</td>\n",
       "      <td>51.324386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19152</th>\n",
       "      <td>one of best</td>\n",
       "      <td>48.932390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14764</th>\n",
       "      <td>love</td>\n",
       "      <td>43.829750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20103</th>\n",
       "      <td>perfect</td>\n",
       "      <td>41.751995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>amazing</td>\n",
       "      <td>40.271278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>39.589658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14801</th>\n",
       "      <td>loved</td>\n",
       "      <td>39.514111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29870</th>\n",
       "      <td>well</td>\n",
       "      <td>38.451604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24828</th>\n",
       "      <td>superb</td>\n",
       "      <td>38.215725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>brilliant</td>\n",
       "      <td>38.141745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8679</th>\n",
       "      <td>favorite</td>\n",
       "      <td>37.800354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14257</th>\n",
       "      <td>life</td>\n",
       "      <td>36.662636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>highly</td>\n",
       "      <td>35.418083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16536</th>\n",
       "      <td>must see</td>\n",
       "      <td>35.275773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>also</td>\n",
       "      <td>34.927260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29228</th>\n",
       "      <td>very</td>\n",
       "      <td>33.866042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19149</th>\n",
       "      <td>one of</td>\n",
       "      <td>33.427117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8586</th>\n",
       "      <td>fantastic</td>\n",
       "      <td>33.146335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>beautifully</td>\n",
       "      <td>33.106315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20115</th>\n",
       "      <td>performance</td>\n",
       "      <td>33.104688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3618</th>\n",
       "      <td>both</td>\n",
       "      <td>32.063273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29343</th>\n",
       "      <td>very well</td>\n",
       "      <td>31.721133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28321</th>\n",
       "      <td>today</td>\n",
       "      <td>31.350023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7804</th>\n",
       "      <td>enjoyed</td>\n",
       "      <td>30.820844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20128</th>\n",
       "      <td>performances</td>\n",
       "      <td>30.688425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>always</td>\n",
       "      <td>30.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29947</th>\n",
       "      <td>well worth</td>\n",
       "      <td>30.359774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>8 10</td>\n",
       "      <td>29.618769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31596</th>\n",
       "      <td>years</td>\n",
       "      <td>29.375083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19721</th>\n",
       "      <td>outstanding</td>\n",
       "      <td>29.215841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11542</th>\n",
       "      <td>highly recommend</td>\n",
       "      <td>29.086497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26557</th>\n",
       "      <td>this great</td>\n",
       "      <td>29.061440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28487</th>\n",
       "      <td>touching</td>\n",
       "      <td>28.927331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10 10</td>\n",
       "      <td>28.664055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31277</th>\n",
       "      <td>world</td>\n",
       "      <td>28.650884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31652</th>\n",
       "      <td>young</td>\n",
       "      <td>28.566383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24610</th>\n",
       "      <td>strong</td>\n",
       "      <td>28.536829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20657</th>\n",
       "      <td>powerful</td>\n",
       "      <td>28.514348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31194</th>\n",
       "      <td>wonderfully</td>\n",
       "      <td>28.452246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29877</th>\n",
       "      <td>well as</td>\n",
       "      <td>28.417276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>definitely</td>\n",
       "      <td>28.268586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20112</th>\n",
       "      <td>perfectly</td>\n",
       "      <td>28.164201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>as well</td>\n",
       "      <td>28.115403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11546</th>\n",
       "      <td>highly recommended</td>\n",
       "      <td>27.925894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>7 10</td>\n",
       "      <td>27.507769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>as well as</td>\n",
       "      <td>27.434764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature  statistic\n",
       "10874               great  73.982716\n",
       "8232            excellent  58.862798\n",
       "31186           wonderful  53.246837\n",
       "3247                 best  53.155283\n",
       "17806             of best  51.324386\n",
       "19152         one of best  48.932390\n",
       "14764                love  43.829750\n",
       "20103             perfect  41.751995\n",
       "1406              amazing  40.271278\n",
       "2936            beautiful  39.589658\n",
       "14801               loved  39.514111\n",
       "29870                well  38.451604\n",
       "24828              superb  38.215725\n",
       "3739            brilliant  38.141745\n",
       "8679             favorite  37.800354\n",
       "14257                life  36.662636\n",
       "11540              highly  35.418083\n",
       "16536            must see  35.275773\n",
       "1224                 also  34.927260\n",
       "29228                very  33.866042\n",
       "19149              one of  33.427117\n",
       "8586            fantastic  33.146335\n",
       "2949          beautifully  33.106315\n",
       "20115         performance  33.104688\n",
       "3618                 both  32.063273\n",
       "29343           very well  31.721133\n",
       "28321               today  31.350023\n",
       "7804              enjoyed  30.820844\n",
       "20128        performances  30.688425\n",
       "1307               always  30.646900\n",
       "29947          well worth  30.359774\n",
       "301                  8 10  29.618769\n",
       "31596               years  29.375083\n",
       "19721         outstanding  29.215841\n",
       "11542    highly recommend  29.086497\n",
       "26557          this great  29.061440\n",
       "28487            touching  28.927331\n",
       "24                  10 10  28.664055\n",
       "31277               world  28.650884\n",
       "31652               young  28.566383\n",
       "24610              strong  28.536829\n",
       "20657            powerful  28.514348\n",
       "31194         wonderfully  28.452246\n",
       "29877             well as  28.417276\n",
       "6334           definitely  28.268586\n",
       "20112           perfectly  28.164201\n",
       "2075              as well  28.115403\n",
       "11546  highly recommended  27.925894\n",
       "289                  7 10  27.507769\n",
       "2076           as well as  27.434764"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at top 50 positive words\n",
    "feature_statistic_df.sort_values(by=\"statistic\", ascending=False).iloc[0:50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65beb5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>bad</td>\n",
       "      <td>-94.755462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31315</th>\n",
       "      <td>worst</td>\n",
       "      <td>-84.764359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29641</th>\n",
       "      <td>waste</td>\n",
       "      <td>-68.997657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>awful</td>\n",
       "      <td>-64.850629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17113</th>\n",
       "      <td>not even</td>\n",
       "      <td>-58.121304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25264</th>\n",
       "      <td>terrible</td>\n",
       "      <td>-57.600314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31306</th>\n",
       "      <td>worse</td>\n",
       "      <td>-53.553785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>boring</td>\n",
       "      <td>-52.661545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24670</th>\n",
       "      <td>stupid</td>\n",
       "      <td>-50.814659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16835</th>\n",
       "      <td>no</td>\n",
       "      <td>-50.695585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17540</th>\n",
       "      <td>nothing</td>\n",
       "      <td>-50.552694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29644</th>\n",
       "      <td>waste of</td>\n",
       "      <td>-50.402360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18668</th>\n",
       "      <td>of worst</td>\n",
       "      <td>-49.201362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11716</th>\n",
       "      <td>horrible</td>\n",
       "      <td>-48.510445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20542</th>\n",
       "      <td>poor</td>\n",
       "      <td>-48.450687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15622</th>\n",
       "      <td>minutes</td>\n",
       "      <td>-47.237653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>even</td>\n",
       "      <td>-45.669468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13504</th>\n",
       "      <td>just</td>\n",
       "      <td>-45.419072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23640</th>\n",
       "      <td>so bad</td>\n",
       "      <td>-44.844958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5936</th>\n",
       "      <td>crap</td>\n",
       "      <td>-44.480812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19199</th>\n",
       "      <td>one of worst</td>\n",
       "      <td>-43.562671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20548</th>\n",
       "      <td>poorly</td>\n",
       "      <td>-43.186878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24867</th>\n",
       "      <td>supposed</td>\n",
       "      <td>-43.143946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>acting</td>\n",
       "      <td>-43.045135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>at all</td>\n",
       "      <td>-42.293576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24868</th>\n",
       "      <td>supposed to</td>\n",
       "      <td>-41.246715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21954</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>-41.040249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20425</th>\n",
       "      <td>plot</td>\n",
       "      <td>-40.879246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5783</th>\n",
       "      <td>could</td>\n",
       "      <td>-40.671791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30629</th>\n",
       "      <td>why</td>\n",
       "      <td>-40.617536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6864</th>\n",
       "      <td>do</td>\n",
       "      <td>-40.385450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22627</th>\n",
       "      <td>script</td>\n",
       "      <td>-40.084532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6898</th>\n",
       "      <td>do not</td>\n",
       "      <td>-39.541573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13944</th>\n",
       "      <td>lame</td>\n",
       "      <td>-39.516830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29647</th>\n",
       "      <td>waste of time</td>\n",
       "      <td>-39.067059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2422</th>\n",
       "      <td>avoid</td>\n",
       "      <td>-38.959725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31328</th>\n",
       "      <td>worst movie</td>\n",
       "      <td>-38.241648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29657</th>\n",
       "      <td>wasted</td>\n",
       "      <td>-37.762129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17497</th>\n",
       "      <td>not waste</td>\n",
       "      <td>-37.274232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>annoying</td>\n",
       "      <td>-37.113367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7359</th>\n",
       "      <td>dull</td>\n",
       "      <td>-36.605027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29649</th>\n",
       "      <td>waste time</td>\n",
       "      <td>-36.465669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6982</th>\n",
       "      <td>do not waste</td>\n",
       "      <td>-35.880702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18746</th>\n",
       "      <td>oh</td>\n",
       "      <td>-35.761045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20501</th>\n",
       "      <td>pointless</td>\n",
       "      <td>-35.559514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15740</th>\n",
       "      <td>money</td>\n",
       "      <td>-35.548515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15481</th>\n",
       "      <td>mess</td>\n",
       "      <td>-35.506881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4943</th>\n",
       "      <td>cheap</td>\n",
       "      <td>-35.372461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14033</th>\n",
       "      <td>laughable</td>\n",
       "      <td>-34.981153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24869</th>\n",
       "      <td>supposed to be</td>\n",
       "      <td>-34.978736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature  statistic\n",
       "2537              bad -94.755462\n",
       "31315           worst -84.764359\n",
       "29641           waste -68.997657\n",
       "2465            awful -64.850629\n",
       "17113        not even -58.121304\n",
       "25264        terrible -57.600314\n",
       "31306           worse -53.553785\n",
       "3598           boring -52.661545\n",
       "24670          stupid -50.814659\n",
       "16835              no -50.695585\n",
       "17540         nothing -50.552694\n",
       "29644        waste of -50.402360\n",
       "18668        of worst -49.201362\n",
       "11716        horrible -48.510445\n",
       "20542            poor -48.450687\n",
       "15622         minutes -47.237653\n",
       "7992             even -45.669468\n",
       "13504            just -45.419072\n",
       "23640          so bad -44.844958\n",
       "5936             crap -44.480812\n",
       "19199    one of worst -43.562671\n",
       "20548          poorly -43.186878\n",
       "24867        supposed -43.143946\n",
       "567            acting -43.045135\n",
       "2169           at all -42.293576\n",
       "24868     supposed to -41.246715\n",
       "21954      ridiculous -41.040249\n",
       "20425            plot -40.879246\n",
       "5783            could -40.671791\n",
       "30629             why -40.617536\n",
       "6864               do -40.385450\n",
       "22627          script -40.084532\n",
       "6898           do not -39.541573\n",
       "13944            lame -39.516830\n",
       "29647   waste of time -39.067059\n",
       "2422            avoid -38.959725\n",
       "31328     worst movie -38.241648\n",
       "29657          wasted -37.762129\n",
       "17497       not waste -37.274232\n",
       "1510         annoying -37.113367\n",
       "7359             dull -36.605027\n",
       "29649      waste time -36.465669\n",
       "6982     do not waste -35.880702\n",
       "18746              oh -35.761045\n",
       "20501       pointless -35.559514\n",
       "15740           money -35.548515\n",
       "15481            mess -35.506881\n",
       "4943            cheap -35.372461\n",
       "14033       laughable -34.981153\n",
       "24869  supposed to be -34.978736"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at bottom 50 words (most negative)\n",
    "feature_statistic_df.sort_values(by=\"statistic\").iloc[0:50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a04c61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11752"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many terms meet the 0.05 significance threshold?\n",
    "len(feature_statistic_df[feature_statistic_df.statistic >= 1.645])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3e090ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           review\n",
       "sentiment        \n",
       "0           62615\n",
       "1           62385"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for class imbalance\n",
    "all_train.groupby([\"sentiment\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93fd2880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2537                     bad\n",
       "31315                  worst\n",
       "10874                  great\n",
       "29641                  waste\n",
       "2465                   awful\n",
       "                ...         \n",
       "24033                sounded\n",
       "16860              no excuse\n",
       "26662    this movie horrible\n",
       "24173                spot on\n",
       "17214            not in good\n",
       "Name: feature, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep the top 2000 terms by magnitude of t-statistic\n",
    "\n",
    "n_terms = 2000\n",
    "\n",
    "feature_statistic_df[\"abs_statistic\"] = abs(feature_statistic_df[\"statistic\"])\n",
    "\n",
    "top_features = feature_statistic_df.sort_values(by=\"abs_statistic\", ascending=False).iloc[:n_terms, 0]\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1229ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['avoid like', 'bad as this', 'bad music', 'could not save',\n",
       "       'crap like this', 'do not waste money', 'easily worst', 'gave 2',\n",
       "       'how not to make', 'instead of 1', 'manos', 'not funny not',\n",
       "       'not waste money on', 'not waste time or', 'this by far worst',\n",
       "       'this drivel', 'this dull', 'this junk', 'this lame',\n",
       "       'this piece of garbage', 'this rubbish', 'this stinker',\n",
       "       'this tripe', 'this turkey', 'this waste', 'this waste of',\n",
       "       'waste time or', 'waste time or money', 'waste time with this',\n",
       "       'worst movie ever made', 'worst movies ever seen'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add any terms that only appeared in positive reviews or only in negative reviews.\n",
    "only_positive = feature_ngrams[np.logical_and((dtm_pos_means > 0), (dtm_neg_means == 0))]\n",
    "only_negative = feature_ngrams[np.logical_and((dtm_pos_means == 0), (dtm_neg_means > 0))]\n",
    "only_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6137619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on edge of seat',\n",
       " 'perfect',\n",
       " 'touch',\n",
       " 'thinking',\n",
       " 'did not even',\n",
       " 'one of greatest',\n",
       " 'film excellent',\n",
       " 'rest of movie',\n",
       " 'to perfection',\n",
       " 'tense',\n",
       " 'film works',\n",
       " 'series',\n",
       " 'with great',\n",
       " 'worst thing',\n",
       " 'dreams',\n",
       " '1',\n",
       " '8 out of',\n",
       " 'be ashamed',\n",
       " 'hours of',\n",
       " 'surprisingly',\n",
       " 'would',\n",
       " 'instead of',\n",
       " 'entire movie',\n",
       " 'did great job',\n",
       " 'sorry',\n",
       " 'basically',\n",
       " 'timeless',\n",
       " 'of most',\n",
       " 'do not',\n",
       " 'painfully',\n",
       " 'meaningless',\n",
       " 'nothing else',\n",
       " 'shows how',\n",
       " '1 10',\n",
       " 'guy',\n",
       " 'may',\n",
       " 'films',\n",
       " 'seemed to',\n",
       " 'ruined',\n",
       " 'in movie',\n",
       " 'at times',\n",
       " 'of worst',\n",
       " 'thing that',\n",
       " 'looks',\n",
       " 'lack',\n",
       " 'awful acting',\n",
       " 'this garbage',\n",
       " 'could not save',\n",
       " 'appalling',\n",
       " 'perfection',\n",
       " 'to make',\n",
       " 'movie no',\n",
       " 'bored',\n",
       " 'even remotely',\n",
       " 'episodes',\n",
       " 'later',\n",
       " 'times',\n",
       " 'impressive',\n",
       " 'begins',\n",
       " 'bother to',\n",
       " 'only good',\n",
       " 'dialog',\n",
       " 'boring movie',\n",
       " 'not good',\n",
       " 'to make movie',\n",
       " 'fails to',\n",
       " 'about only',\n",
       " 'movie would',\n",
       " 'please do',\n",
       " 'stupid movie',\n",
       " 'worst ever seen',\n",
       " 'man',\n",
       " 'f',\n",
       " 'then',\n",
       " 'heartbreaking',\n",
       " 'trash',\n",
       " 'really enjoyed this',\n",
       " 'spent',\n",
       " 'poor acting',\n",
       " 'or something',\n",
       " 'avoid at',\n",
       " 'to begin',\n",
       " 'no point',\n",
       " 'porno',\n",
       " 'half way through',\n",
       " 'but great',\n",
       " 'seriously',\n",
       " 'filmmakers',\n",
       " 'joy to',\n",
       " 'this best',\n",
       " 'still',\n",
       " 'should be ashamed',\n",
       " 'of junk',\n",
       " 'this horrible',\n",
       " 'horrid',\n",
       " '90 minutes of',\n",
       " 'no character',\n",
       " 'worst films ever',\n",
       " 'one of most',\n",
       " 'first saw this movie',\n",
       " '4 out of 10',\n",
       " 'history',\n",
       " 'do yourself',\n",
       " 'uwe boll',\n",
       " 'had',\n",
       " 'feel good',\n",
       " 'hours',\n",
       " 'not help',\n",
       " 'not recommend',\n",
       " 'mean come',\n",
       " 'complete waste of',\n",
       " 'bad this movie',\n",
       " 'bottom',\n",
       " 'disgusting',\n",
       " 'vhs',\n",
       " 'pretty bad',\n",
       " 'could been so much',\n",
       " 'sit through this',\n",
       " 'fast forward',\n",
       " 'view',\n",
       " 'sort of',\n",
       " 'finds',\n",
       " 'why does',\n",
       " 'truly awful',\n",
       " 'would rather',\n",
       " 'there',\n",
       " 'breathtaking',\n",
       " 'cinema',\n",
       " '9 out of',\n",
       " 'everyday',\n",
       " 'to recommend',\n",
       " 'sweet',\n",
       " 'no',\n",
       " 'do not bother',\n",
       " 'movie had',\n",
       " 'skip this',\n",
       " 'drama',\n",
       " 'hype',\n",
       " 'sequel',\n",
       " 'this movie bad',\n",
       " '10 minutes',\n",
       " 'do not bother with',\n",
       " 'complex',\n",
       " 'all ages',\n",
       " 'best movie',\n",
       " 'supporting cast',\n",
       " 'writers',\n",
       " 'not be disappointed',\n",
       " 'at best',\n",
       " 'tragedy',\n",
       " 'waste of time money',\n",
       " 'matthau',\n",
       " 'been better',\n",
       " 'little film',\n",
       " 'apparent reason',\n",
       " 'love this movie',\n",
       " 'instead of 1',\n",
       " 'this stinker',\n",
       " 'not much',\n",
       " 'as well',\n",
       " 'not very good',\n",
       " 'to see again',\n",
       " 'not even',\n",
       " 'worst movies',\n",
       " 'best performances',\n",
       " 'touched',\n",
       " 'bad script',\n",
       " 'this piece of',\n",
       " 'not perfect',\n",
       " 'only complaint',\n",
       " 'unfunny',\n",
       " 'intense',\n",
       " 'watch again',\n",
       " 'job of',\n",
       " 'saw this',\n",
       " 'well',\n",
       " 'nominated',\n",
       " 'of best',\n",
       " 'memories',\n",
       " 'spectacular',\n",
       " 'notch',\n",
       " 'stay away',\n",
       " 'most annoying',\n",
       " 'to write',\n",
       " 'save',\n",
       " 'at least',\n",
       " 'as worst',\n",
       " 'redeeming',\n",
       " 'animation',\n",
       " 'this one of',\n",
       " 'such bad',\n",
       " 'gets worse',\n",
       " 'just great',\n",
       " 'piece',\n",
       " 'so badly',\n",
       " 'performance of',\n",
       " 'obvious',\n",
       " 'french',\n",
       " 'of best ever',\n",
       " 'liked',\n",
       " 'love with',\n",
       " 'want',\n",
       " \"today's\",\n",
       " 'could been',\n",
       " 'together',\n",
       " 'loves',\n",
       " 'about any of',\n",
       " 'unbelievable',\n",
       " 'relationships',\n",
       " 'movie awful',\n",
       " 'life of',\n",
       " 'jean',\n",
       " 'wasted in',\n",
       " 'genuine',\n",
       " 'movie do not',\n",
       " 'paid',\n",
       " 'travesty',\n",
       " 'just',\n",
       " 'waste',\n",
       " 'that supposed to',\n",
       " 'run around',\n",
       " 'keeps',\n",
       " 'highly recommended',\n",
       " 'awesome',\n",
       " 'total',\n",
       " 'either',\n",
       " 'film so bad',\n",
       " 'thing',\n",
       " 'brought to',\n",
       " 'treat',\n",
       " 'promising',\n",
       " 'dreck',\n",
       " 'of this movie',\n",
       " 'would been better',\n",
       " 'enjoyable',\n",
       " 'poorly made',\n",
       " 'this pile of',\n",
       " 'muddled',\n",
       " 'release',\n",
       " 'bad films',\n",
       " 'horror movie',\n",
       " 'of worst films',\n",
       " 'enjoyed',\n",
       " 'not recommend this',\n",
       " 'camera',\n",
       " 'ripped off',\n",
       " 'about this',\n",
       " 'terrible acting',\n",
       " 'not seen',\n",
       " 'will love',\n",
       " 'money on',\n",
       " 'killed',\n",
       " 'train wreck',\n",
       " 'loved',\n",
       " 'brutal',\n",
       " 'first saw',\n",
       " 'cgi',\n",
       " 'ages',\n",
       " 'wretched',\n",
       " 'movie sucks',\n",
       " 'this disaster',\n",
       " 'favorite',\n",
       " 'soundtrack',\n",
       " 'unnecessary',\n",
       " 'poorly executed',\n",
       " 'below average',\n",
       " 'but just',\n",
       " 'william',\n",
       " 'did not work',\n",
       " 'else to',\n",
       " 'terrible script',\n",
       " 'tells story',\n",
       " 'marriage',\n",
       " 'beautiful',\n",
       " 'many',\n",
       " 'sadly',\n",
       " 'why',\n",
       " 'even worse',\n",
       " 'this piece',\n",
       " 'hoping',\n",
       " 'stinker',\n",
       " 'may not',\n",
       " 'best of',\n",
       " 'raw',\n",
       " 'powerful',\n",
       " 'anything to',\n",
       " 'even',\n",
       " 'wonderful movie',\n",
       " 'to all',\n",
       " 'stick',\n",
       " 'how awful',\n",
       " 'embarrassment',\n",
       " 'sadness',\n",
       " 'only saving',\n",
       " 'name',\n",
       " 'in many ways',\n",
       " 'pleasure',\n",
       " 'incredible',\n",
       " 'mean come on',\n",
       " 'away from this',\n",
       " 'helps',\n",
       " 'problem',\n",
       " 'barely',\n",
       " 'at time',\n",
       " 'gem of',\n",
       " 'enough',\n",
       " 'bad this',\n",
       " 'to laugh at',\n",
       " 'this supposed',\n",
       " 'unconvincing',\n",
       " 'insult to',\n",
       " 'very boring',\n",
       " 'horror',\n",
       " 'crappy',\n",
       " '8 out of 10',\n",
       " 'paul',\n",
       " 'watching this movie',\n",
       " 'embarrassed',\n",
       " 'hours of life',\n",
       " 'so called',\n",
       " 'bother',\n",
       " 'that would',\n",
       " 'outer space',\n",
       " 'bad really',\n",
       " 'performances as',\n",
       " 'avoid at all costs',\n",
       " 'movies',\n",
       " 'finest',\n",
       " 'human',\n",
       " 'complete waste',\n",
       " 'rewarding',\n",
       " 'really',\n",
       " 'so',\n",
       " 'paint dry',\n",
       " 'just right',\n",
       " 'easy to',\n",
       " 'say about',\n",
       " 'what with',\n",
       " 'good thing',\n",
       " 'that will',\n",
       " 'sharp',\n",
       " 'thought provoking',\n",
       " 'to believe that',\n",
       " 'then there',\n",
       " 'sort',\n",
       " 'including',\n",
       " 'to waste',\n",
       " 'sake',\n",
       " 'cheap',\n",
       " 'better',\n",
       " 'thing about this',\n",
       " 'award',\n",
       " 'delightful',\n",
       " 'watchable',\n",
       " 'of war',\n",
       " 'what thinking',\n",
       " 'greatest',\n",
       " 'relationship',\n",
       " 'first off',\n",
       " 'sorry for',\n",
       " 'too many',\n",
       " 'wasted time',\n",
       " 'inept',\n",
       " 'moronic',\n",
       " 'available on dvd',\n",
       " 'tears',\n",
       " 'laugh',\n",
       " 'debut',\n",
       " 'father',\n",
       " 'creature',\n",
       " 'performance as',\n",
       " 'day',\n",
       " 'do not see this',\n",
       " 'available',\n",
       " 'if',\n",
       " 'hard to be',\n",
       " 'mesmerizing',\n",
       " 'only thing that',\n",
       " 'pretty',\n",
       " 'home',\n",
       " 'pile',\n",
       " 'tremendous',\n",
       " 'rip',\n",
       " 'to sit through this',\n",
       " 'just boring',\n",
       " 'cast',\n",
       " 'fell asleep',\n",
       " 'not care about',\n",
       " 'stay away from this',\n",
       " 'sensitive',\n",
       " 'great fun',\n",
       " 'should been',\n",
       " 'those who',\n",
       " 'waste time with',\n",
       " 'best of all',\n",
       " 'get',\n",
       " 'for no reason',\n",
       " 'film very',\n",
       " 'this poor',\n",
       " 'waste of time',\n",
       " 'very',\n",
       " 'cannot wait',\n",
       " 'fails miserably',\n",
       " '10 out',\n",
       " 'really wanted to',\n",
       " 'this worst',\n",
       " 'disaster',\n",
       " 'year olds',\n",
       " 'favourite',\n",
       " 'making',\n",
       " 'there no plot',\n",
       " 'of best films',\n",
       " 'james',\n",
       " 'bunch of',\n",
       " 'definitely recommend',\n",
       " 'of worst movies ever',\n",
       " 'excellent movie',\n",
       " 'creates',\n",
       " 'scary',\n",
       " 'warn',\n",
       " '7 10',\n",
       " 'to sit',\n",
       " 'half',\n",
       " 'whole',\n",
       " 'appreciate',\n",
       " 'but this movie',\n",
       " 'problem with',\n",
       " 'no idea',\n",
       " 'that supposed',\n",
       " 'embarrassingly',\n",
       " 'remotely',\n",
       " 'lacked',\n",
       " 'very well done',\n",
       " 'blend of',\n",
       " 'consequences',\n",
       " 'well written',\n",
       " 'waste time or',\n",
       " 'retarded',\n",
       " 'not miss this',\n",
       " 'bothered',\n",
       " 'thanks',\n",
       " 'that this',\n",
       " 'movie not worth',\n",
       " 'unoriginal',\n",
       " 'porn',\n",
       " 'let',\n",
       " 'how not to make',\n",
       " '1 out of',\n",
       " 'may not be',\n",
       " 'story told',\n",
       " 'nowhere',\n",
       " 'quite',\n",
       " 'zero',\n",
       " 'just terrible',\n",
       " 'far too',\n",
       " 'this crap',\n",
       " 'this great movie',\n",
       " 'uplifting',\n",
       " 'trying to be',\n",
       " 'going for',\n",
       " 'music by',\n",
       " 'money on this',\n",
       " 'acting superb',\n",
       " 'unfortunately this',\n",
       " 'horrible acting',\n",
       " 'in many',\n",
       " 'only',\n",
       " 'just does not',\n",
       " 'avoid this movie',\n",
       " 'plot holes',\n",
       " 'emotional',\n",
       " 'really enjoyed',\n",
       " 'worst film ever',\n",
       " 'this junk',\n",
       " 'takes',\n",
       " 'but even',\n",
       " 'funny not',\n",
       " 'would be',\n",
       " 'disappointing',\n",
       " 'repetitive',\n",
       " 'that supposed to be',\n",
       " 'beauty of',\n",
       " 'effectively',\n",
       " 'best',\n",
       " 'good idea',\n",
       " 'fell in love',\n",
       " 'dumb',\n",
       " 'see for',\n",
       " 'this gem',\n",
       " 'make any',\n",
       " 'badly written',\n",
       " 'complete',\n",
       " 'explain',\n",
       " 'come up',\n",
       " 'ensemble',\n",
       " 'camcorder',\n",
       " 'for everyone',\n",
       " 'in first place',\n",
       " 'for best',\n",
       " 'growing up',\n",
       " 'contrast',\n",
       " 'plus side',\n",
       " 'this supposed to',\n",
       " 'performance',\n",
       " 'flat',\n",
       " 'come on',\n",
       " 'this turkey',\n",
       " 'delivers',\n",
       " 'am afraid',\n",
       " 'important',\n",
       " 'trying to',\n",
       " 'to be worst',\n",
       " 'this very',\n",
       " 'by',\n",
       " 'holds',\n",
       " 'job as',\n",
       " 'achievement',\n",
       " 'acting poor',\n",
       " 'worst movie ever seen',\n",
       " 'mystery science theater',\n",
       " 'enjoy',\n",
       " 'worst films',\n",
       " 'movie 2',\n",
       " 'chance to',\n",
       " 'unless',\n",
       " '90',\n",
       " 'would love to',\n",
       " 'ashamed',\n",
       " 'reality',\n",
       " 'attempt',\n",
       " 'more like',\n",
       " 'appreciated',\n",
       " 'just plain',\n",
       " 'loneliness',\n",
       " 'something',\n",
       " 'painful to',\n",
       " 'sitting through',\n",
       " 'to life',\n",
       " 'awful film',\n",
       " 'interesting but',\n",
       " 'bad but',\n",
       " 'horrendous',\n",
       " 'very different',\n",
       " 'david',\n",
       " 'do not care',\n",
       " 'one of worst',\n",
       " 'existent',\n",
       " 'this movie had',\n",
       " 'without any',\n",
       " 'no apparent reason',\n",
       " '2 out',\n",
       " 'haunting',\n",
       " 'bit',\n",
       " 'probably worst',\n",
       " 'no redeeming',\n",
       " 'true',\n",
       " 'recommend',\n",
       " 'good about',\n",
       " 'not even bother',\n",
       " 'like watching',\n",
       " 'original',\n",
       " 'shoot',\n",
       " 'best films',\n",
       " 'total waste',\n",
       " 'some great',\n",
       " 'to cash in on',\n",
       " 'fabulous',\n",
       " 'mess of',\n",
       " 'bad enough',\n",
       " 'childhood',\n",
       " 'episode',\n",
       " 'to be funny',\n",
       " 'waste time with this',\n",
       " 'unfortunately',\n",
       " 'suppose',\n",
       " 'always',\n",
       " 'small',\n",
       " '4 out of',\n",
       " 'science theater',\n",
       " 'avoid this',\n",
       " 'of garbage',\n",
       " 'attempt to',\n",
       " 'wondering',\n",
       " 'lack of',\n",
       " 'as always',\n",
       " 'unintentionally funny',\n",
       " 'great acting',\n",
       " 'not miss',\n",
       " 'underrated',\n",
       " 'great to',\n",
       " 'what makes this',\n",
       " 'effective',\n",
       " 'be good',\n",
       " 'this drivel',\n",
       " 'subtle',\n",
       " 'money back',\n",
       " 'point',\n",
       " 'producers',\n",
       " 'not work',\n",
       " 'stock footage',\n",
       " 'costs',\n",
       " 'others',\n",
       " 'not worth time',\n",
       " 'favorites',\n",
       " 'does great',\n",
       " 'worst movie',\n",
       " 'see again',\n",
       " 'better off',\n",
       " 'lame',\n",
       " 'this great film',\n",
       " 'also very good',\n",
       " 'visual',\n",
       " 'seen worse',\n",
       " '9 out of 10',\n",
       " 'liked this',\n",
       " 'brilliant',\n",
       " 'ripped',\n",
       " 'alright',\n",
       " 'idea but',\n",
       " 'of love',\n",
       " 'of all time',\n",
       " 'believable',\n",
       " 'very well',\n",
       " 'say',\n",
       " 'of fun',\n",
       " 'still great',\n",
       " 'academy award',\n",
       " 'dull',\n",
       " 'other than',\n",
       " 'badly acted',\n",
       " 'if really',\n",
       " 'supposed',\n",
       " 'also great',\n",
       " 'must see',\n",
       " 'would love',\n",
       " 'but also',\n",
       " 'but at least',\n",
       " '3 out of 10',\n",
       " 'only thing',\n",
       " 'saving grace',\n",
       " 'forgettable',\n",
       " 'wanted',\n",
       " 'of human',\n",
       " 'not even worth',\n",
       " 'do not miss this',\n",
       " 'adds',\n",
       " 'this waste',\n",
       " 'in lives',\n",
       " 'attempt at',\n",
       " 'trying',\n",
       " 'did not care',\n",
       " 'easy',\n",
       " 'below',\n",
       " 'favorite movies',\n",
       " 'just do',\n",
       " 'this lame',\n",
       " 'going to',\n",
       " 'this must see',\n",
       " 'nothing to',\n",
       " 'sense at',\n",
       " 'moving',\n",
       " 'sense of',\n",
       " 'enjoyed this movie',\n",
       " 'job in',\n",
       " 'someone',\n",
       " 'inane',\n",
       " 'will',\n",
       " 'really great',\n",
       " 'okay',\n",
       " 'wanted to like this',\n",
       " 'tells',\n",
       " 'only good thing about',\n",
       " 'twists',\n",
       " 'no wonder',\n",
       " 'even close to',\n",
       " 'wonderfully',\n",
       " 'or money',\n",
       " 'romance',\n",
       " 'entertaining',\n",
       " 'there absolutely',\n",
       " 'certainly',\n",
       " 'not very',\n",
       " 'along with',\n",
       " 'complete waste of time',\n",
       " 'own',\n",
       " 'worst movie ever made',\n",
       " 'just not',\n",
       " 'just did',\n",
       " 'memorable',\n",
       " 'waste money on',\n",
       " 'painful to watch',\n",
       " 'nothing to do',\n",
       " 'time with this',\n",
       " 'wonderful as',\n",
       " 'even better',\n",
       " 'fails',\n",
       " 'for first time',\n",
       " 'worst film ever seen',\n",
       " 'movie ever seen',\n",
       " 'worst ever',\n",
       " 'am sorry but',\n",
       " 'avoid like',\n",
       " 'good as',\n",
       " 'power of',\n",
       " 'total waste of',\n",
       " 'not watch this movie',\n",
       " 'pitiful',\n",
       " 'seagal',\n",
       " 'no story',\n",
       " 'will never get back',\n",
       " 'portrait of',\n",
       " 'least',\n",
       " 'funny or',\n",
       " 'nothing',\n",
       " 'mother',\n",
       " 'as good as',\n",
       " 'not make',\n",
       " 'worst movies ever',\n",
       " 'dvd',\n",
       " 'should never',\n",
       " 'yeah',\n",
       " 'none of',\n",
       " 'waste money',\n",
       " 'really liked',\n",
       " 'if want to',\n",
       " 'movie 1',\n",
       " 'one of all time',\n",
       " 'traditional',\n",
       " 'laughable',\n",
       " 'for first',\n",
       " 'this movie could',\n",
       " 'captured',\n",
       " 'magnificent',\n",
       " 'available on',\n",
       " 'very much',\n",
       " 'where to begin',\n",
       " 'had potential',\n",
       " 'better than this',\n",
       " 'deeply',\n",
       " 'cash in',\n",
       " 'this mess',\n",
       " 'excellent film',\n",
       " 'many years',\n",
       " 'masterpiece',\n",
       " 'how bad this',\n",
       " 'but fails',\n",
       " 'not watch',\n",
       " 'zombie',\n",
       " 'one good',\n",
       " 'fascinating',\n",
       " 'shelf',\n",
       " 'of barrel',\n",
       " 'bad as this',\n",
       " 'renting',\n",
       " 'insulting',\n",
       " 'recommend this',\n",
       " 'definitely worth',\n",
       " 'story of',\n",
       " 'between',\n",
       " 'atmosphere of',\n",
       " 'theme',\n",
       " 'apparently',\n",
       " 'beauty',\n",
       " 'images',\n",
       " 'incomprehensible',\n",
       " 'this stupid',\n",
       " 'whole movie',\n",
       " 'york',\n",
       " 'not funny',\n",
       " 'what point',\n",
       " 'portrait',\n",
       " 'tiresome',\n",
       " 'not waste time or',\n",
       " 'both',\n",
       " 'first time',\n",
       " 'seemed',\n",
       " 'avoid this one',\n",
       " 'this piece of crap',\n",
       " 'marvelous',\n",
       " 'be seen',\n",
       " 'why do',\n",
       " 'age',\n",
       " 'too',\n",
       " 'way too',\n",
       " 'not waste time with',\n",
       " 'looked like',\n",
       " 'fell in love with',\n",
       " 'time money',\n",
       " 'even funny',\n",
       " 'on cover',\n",
       " 'waste of',\n",
       " 'this movie terrible',\n",
       " 'of family',\n",
       " 'not waste money on',\n",
       " 'killer',\n",
       " 'film just',\n",
       " 'classic',\n",
       " 'there absolutely no',\n",
       " 'or',\n",
       " 'paced',\n",
       " 'looked',\n",
       " 'tried',\n",
       " 'sense at all',\n",
       " '8',\n",
       " '8 10',\n",
       " 'wooden',\n",
       " 'painful',\n",
       " 'cast excellent',\n",
       " 'especially',\n",
       " 'lines',\n",
       " 'decent',\n",
       " 'flawless',\n",
       " 'brilliant as',\n",
       " 'simplicity',\n",
       " 'redeeming quality',\n",
       " 'waste time on',\n",
       " 'not waste time',\n",
       " 'begin with',\n",
       " 'terrible movie',\n",
       " 'this waste of',\n",
       " 'some guy',\n",
       " 'avoid at all',\n",
       " 'tries to',\n",
       " 'edge of seat',\n",
       " 'heart warming',\n",
       " 'gives',\n",
       " 'poorly acted',\n",
       " 'one of worst films',\n",
       " 'shallow',\n",
       " 'failed',\n",
       " '9',\n",
       " 'whole thing',\n",
       " 'this movie so bad',\n",
       " 'how did',\n",
       " 'instead',\n",
       " 'explores',\n",
       " 'ridiculous',\n",
       " 'theater 3000',\n",
       " 'decided to',\n",
       " 'this bad',\n",
       " '9 out',\n",
       " 'extremely well',\n",
       " 'brilliantly',\n",
       " 'nothing more',\n",
       " 'so bad good',\n",
       " 'in role',\n",
       " 'of own',\n",
       " 'works',\n",
       " 'cliché',\n",
       " 'running',\n",
       " 'affection',\n",
       " 'any real',\n",
       " 'all great',\n",
       " 'out',\n",
       " 'on edge of',\n",
       " 'riveting',\n",
       " 'do not miss',\n",
       " 'reason',\n",
       " 'louis',\n",
       " 'mildly',\n",
       " 'not bother with',\n",
       " 'movie not even',\n",
       " 'poorly',\n",
       " 'be worst',\n",
       " 'idiotic',\n",
       " 'doing',\n",
       " 'best performance',\n",
       " 'in love',\n",
       " 'disjointed',\n",
       " 'hackneyed',\n",
       " 'shoddy',\n",
       " 'how bad this movie',\n",
       " 'awfulness',\n",
       " 'unless want to',\n",
       " 'portrayal of',\n",
       " 'problem that',\n",
       " 'another great',\n",
       " 'could not even',\n",
       " 'manos',\n",
       " 'music',\n",
       " 'crafted',\n",
       " 'not worth watching',\n",
       " 'refreshing',\n",
       " 'superb',\n",
       " 'new york',\n",
       " 'lives',\n",
       " 'will not be disappointed',\n",
       " 'inaccurate',\n",
       " '0',\n",
       " 'instead get',\n",
       " 'worse',\n",
       " 'friendship',\n",
       " 'best film',\n",
       " 'seemed like',\n",
       " 'unbelievably',\n",
       " 'even for',\n",
       " 'romantic',\n",
       " 'not enough',\n",
       " 'satisfying',\n",
       " 'whole family',\n",
       " 'around in',\n",
       " 'simple',\n",
       " 'provoking',\n",
       " 'simply not',\n",
       " 'editing',\n",
       " 'like',\n",
       " 'wasted',\n",
       " 'nothing to do with',\n",
       " 'lacks',\n",
       " 'drags',\n",
       " 'elegant',\n",
       " 'to laugh',\n",
       " 'won',\n",
       " 'save yourself',\n",
       " 'this one of worst',\n",
       " 'this just',\n",
       " 'love story',\n",
       " '3',\n",
       " 'in own',\n",
       " 'uwe',\n",
       " 'of clichés',\n",
       " 'possibly worst',\n",
       " 'like bad',\n",
       " 'zombies',\n",
       " 'young',\n",
       " 'each',\n",
       " 'utter',\n",
       " 'somebody',\n",
       " 'not funny at',\n",
       " 'just did not',\n",
       " 'feelings',\n",
       " 'would think',\n",
       " 'call that',\n",
       " 'rest',\n",
       " 'one of favorite',\n",
       " '4 out',\n",
       " 'utterly',\n",
       " 'no chemistry',\n",
       " 'must see for',\n",
       " 'worst film',\n",
       " 'just awful',\n",
       " 'well worth',\n",
       " 'movie could',\n",
       " 'devotion',\n",
       " 'to be comedy',\n",
       " 'of time money',\n",
       " 'through this',\n",
       " 'some',\n",
       " 'meets',\n",
       " 'clichés',\n",
       " 'also excellent',\n",
       " 'cannot believe',\n",
       " 'positive',\n",
       " 'films of',\n",
       " 'garbage',\n",
       " 'to believe',\n",
       " 'bad as',\n",
       " 'turd',\n",
       " 'continuity',\n",
       " 'intensity',\n",
       " 'be one of worst',\n",
       " 'failure',\n",
       " 'cover',\n",
       " 'to do with',\n",
       " 'fake',\n",
       " 'very bad',\n",
       " 'compelling',\n",
       " 'unrealistic',\n",
       " 'great film',\n",
       " 'cheesy',\n",
       " 'like plague',\n",
       " 'atrocious',\n",
       " '10 out of 10',\n",
       " 'coherent',\n",
       " 'stock',\n",
       " 'late',\n",
       " 'trite',\n",
       " 'poorly done',\n",
       " 'halfway',\n",
       " 'shines',\n",
       " 'suck',\n",
       " 'how not',\n",
       " 'still very',\n",
       " 'pile of',\n",
       " 'irritating',\n",
       " 'of trash',\n",
       " 'at same',\n",
       " 'of young',\n",
       " 'credibility',\n",
       " 'favorite of',\n",
       " 'personal',\n",
       " 'for no apparent reason',\n",
       " 'just does not work',\n",
       " 'want to',\n",
       " 'all costs',\n",
       " 'rented',\n",
       " 'had high',\n",
       " 'for this',\n",
       " 'bother with this',\n",
       " 'this pile',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features = list(set(top_features.tolist() + only_positive.tolist() + only_negative.tolist()))\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c40ab8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0fc624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the top features to file\n",
    "np.savetxt(f\"top_{len(top_features)}_features.txt\", top_features, fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e6b263",
   "metadata": {},
   "source": [
    "### Find best ridge-regression model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "321d893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the full training set using the top 2000 features\n",
    "\n",
    "top_feature_vectorizer = CountVectorizer(\n",
    "    vocabulary=top_features,          # The top 200 features\n",
    "    stop_words=stopwords,             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                     # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r\"\\b[\\w+\\|']+\\b\"    # Use word tokenizer, but don't split on apostrophes\n",
    ")\n",
    "\n",
    "dtm_vocab_train = top_feature_vectorizer.fit_transform(all_train[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a403fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f577ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = LogisticRegressionCV(Cs=10, cv=5, penalty=\"l2\", scoring=\"roc_auc\", max_iter=100000, random_state=SEED, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51d55b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_y = all_train[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e08870de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegressionCV(cv=5, max_iter=100000, random_state=4031,\n",
       "                     scoring=&#x27;roc_auc&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegressionCV</label><div class=\"sk-toggleable__content\"><pre>LogisticRegressionCV(cv=5, max_iter=100000, random_state=4031,\n",
       "                     scoring=&#x27;roc_auc&#x27;, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegressionCV(cv=5, max_iter=100000, random_state=4031,\n",
       "                     scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(dtm_vocab_train, all_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c43380e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166.81005372000558"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_C = grid_search.C_[0]\n",
    "best_C   # best_C is 166.81005372"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4dbee9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([[0.90935775, 0.94354946, 0.96294727, 0.97165266, 0.9748084 ,\n",
       "         0.97552203, 0.97560437, 0.97560651, 0.97560641, 0.97560652],\n",
       "        [0.91301104, 0.9465028 , 0.96414   , 0.97174582, 0.9744883 ,\n",
       "         0.97511399, 0.97520503, 0.97521818, 0.97521824, 0.97521816],\n",
       "        [0.91137743, 0.94582756, 0.96427532, 0.9722274 , 0.97494584,\n",
       "         0.97547194, 0.97554306, 0.97555951, 0.97555951, 0.97555951],\n",
       "        [0.91270893, 0.9457004 , 0.96344614, 0.97148508, 0.97442537,\n",
       "         0.97511311, 0.97520429, 0.97520059, 0.97520056, 0.9752005 ],\n",
       "        [0.91288587, 0.94584362, 0.96394406, 0.97208492, 0.97515224,\n",
       "         0.97586396, 0.97595001, 0.97593079, 0.97593079, 0.97593085]])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1eb3f",
   "metadata": {},
   "source": [
    "### How well do the 2000-ish terms predict movie review sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca96ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, preprocess training and test sets from each split. This way it's faster to try multiple model settings.\n",
    "\n",
    "train_X_preprocessed = []\n",
    "train_y_list = []\n",
    "\n",
    "test_X_preprocessed = []\n",
    "test_y_list = []\n",
    "\n",
    "for i in range(len(train_datasets)):\n",
    "\n",
    "    train_X_preprocessed.append(top_feature_vectorizer.fit_transform(preprocess_reviews(train_datasets[i][\"review\"])))\n",
    "    train_y_list.append(train_datasets[i][\"sentiment\"])\n",
    "    \n",
    "    test_X_preprocessed.append(top_feature_vectorizer.transform(preprocess_reviews(test_datasets[i][\"review\"])))\n",
    "    test_y_list.append(test_ys[i][\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efbe74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62b84121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of split 1: 0.956204704468669\n",
      "AUC of split 2: 0.9564355005818861\n",
      "AUC of split 3: 0.9546606942152958\n",
      "AUC of split 4: 0.9565497321918287\n",
      "AUC of split 5: 0.9548672825223585\n"
     ]
    }
   ],
   "source": [
    "pred_y = []\n",
    "auc_scores = []\n",
    "\n",
    "for i in range(len(train_X_preprocessed)):\n",
    "    model = LogisticRegression(C=best_C, penalty=\"l2\", max_iter=100000, random_state=SEED)\n",
    "    \n",
    "    model.fit(train_X_preprocessed[i], train_y_list[i])\n",
    "    \n",
    "    pred_y.append(model.predict_proba(test_X_preprocessed[i])[:, 1])  # Predict probabilities for class 1 (positive review)\n",
    "    \n",
    "    auc_score = roc_auc_score(test_y_list[i], pred_y[i])\n",
    "    auc_scores.append(auc_score)\n",
    "    \n",
    "    print(f\"AUC of split {i+1}: {auc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7470e10",
   "metadata": {},
   "source": [
    "## Reduce Vocabulary\n",
    "\n",
    "Need to reduce vocabulary to under 100 feature for full points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf5b135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1612471",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Ignore code below this line</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9045189",
   "metadata": {},
   "source": [
    "### Try Support Vector Classifier with kernel\n",
    "\n",
    "See if the incorporation of interaction terms increases AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df2100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03dc8d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try RBF kernel\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC(kernel=\"rbf\", degree=3, probability=True, random_state=SEED)\n",
    "model.fit(train_X_preprocessed[0], train_y_list[0])\n",
    "pred_y = model.predict_proba(test_X_preprocessed[0])[:, 1]  # Predict probabilities for class 1 (positive review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aff1c638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.25840412e-01, 5.61584751e-02, 9.54795041e-01, ...,\n",
       "       9.99999807e-01, 1.66210730e-01, 6.08238567e-05])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61b06493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9550786512149552"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_y_list[0], pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00f64494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try polynomial kernel\n",
    "\n",
    "model = svm.SVC(kernel=\"poly\", degree=3, probability=True, random_state=SEED)\n",
    "model.fit(train_X_preprocessed[0], train_y_list[0])\n",
    "pred_y = model.predict_proba(test_X_preprocessed[0])[:, 1]  # Predict probabilities for class 1 (positive review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c650144d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9095020097725768"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_y_list[0], pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da58ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try grid search over SVC regularization parameter\n",
    "# *** GRID SEARCH RUNS > 18 HOURS WITHOUT COMPLETING ***\n",
    "\n",
    "#svc_model = svm.SVC(kernel=\"rbf\", probability=True, random_state=SEED)\n",
    "\n",
    "#param_grid = {'C': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "#grid_search = GridSearchCV(estimator=svc_model, param_grid=param_grid, cv=5, scoring=\"roc_auc\")\n",
    "#grid_search.fit(dtm_vocab_train, all_train_y)\n",
    "#best_svc_params = grid_search.best_estimator_\n",
    "#best_svc_C = best_lasso_params.get_params()[\"C\"]\n",
    "\n",
    "\n",
    "\n",
    "### Reduce the number of terms to under 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5e0c0",
   "metadata": {},
   "source": [
    "## Attempt at lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add lemmatizer to preprocessing.\n",
    "# Code taken from StackOverflow post: https://stackoverflow.com/questions/47423854/sklearn-adding-lemmatizer-to-countvectorizer\n",
    "\n",
    "# To make this work, need to deal with punctuation. Also need to provide POS tags, like here:\n",
    "# https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68aa2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_vectorizer = CountVectorizer(\n",
    "    lowercase=True,                   # Convert to lowercase\n",
    "    tokenizer=LemmaTokenizer(),       # Lemmatization\n",
    "    stop_words=stopwords,             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                     # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r\"\\b[\\w+\\|']+\\b\"    # Use word tokenizer\n",
    ")\n",
    "\n",
    "lemma_dtm_train = lemma_vectorizer.fit_transform(train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000fccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some of the ngrams identified\n",
    "lemma_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7335d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the number of ngrams\n",
    "lemma_vectorizer.get_feature_names_out().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2764e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode as Unicode\n",
    "feature_names = [l.encode(\"utf-8\") for l in lemma_vectorizer.get_feature_names_out()]\n",
    "\n",
    "# Output features to file\n",
    "with open(\"split_1/lemma_vectorizer_features.txt\", \"wb\") as f:\n",
    "    for l in feature_names:\n",
    "        f.write(b'%s\\n'%l)\n",
    "\n",
    "np.savetxt(\"split_1/lemma_vectorizer_features.txt\", lemma_vectorizer.get_feature_names_out(), fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d680c",
   "metadata": {},
   "source": [
    "## Try stemming words before computing their predictive power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0879f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stem = train_datasets[3]\n",
    "# Remove HTML tags\n",
    "train_stem[\"review\"] = train_stem[\"review\"].str.replace('<.*?>', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd647c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efef427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctulation besides apostrophes.\n",
    "# Regular expression from StackOverflow post:\n",
    "# https://stackoverflow.com/questions/59877761/how-to-strip-string-from-punctuation-except-apostrophes-for-nlp\n",
    "train_stem[\"review\"] = train_stem[\"review\"].str.replace('[^\\w\\d\\s\\']+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02962f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try whitespace tokenizer to avoid splitting words on apostrophes\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tk = WhitespaceTokenizer()\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf7960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk import word_tokenize\n",
    "\n",
    "for i in range(len(train_stem)):\n",
    "    train_stem.iloc[i, 2] = \" \".join([p_stemmer.stem(review_token) for review_token in tk.tokenize(train_stem.iloc[i, 2])\n",
    "                                      if review_token not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5a4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaad5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorterStemmer class to add to preprocessing.\n",
    "class StemmerPorter(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = PorterStemmer()\n",
    "        self.tk = WhitespaceTokenizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.stem(t) for t in self.tk.tokenize(articles) if t not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a3a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_vectorizer = CountVectorizer(\n",
    "    lowercase=True,                   # Convert to lowercase\n",
    "    tokenizer=StemmerPorter(),        # Stemming\n",
    "    stop_words=stopwords,             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                     # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r'[^\\w\\d\\s\\']+'     # Keep apostrophes while removing other punctuation\n",
    ")\n",
    "\n",
    "dtm_train_stem = stem_vectorizer.fit_transform(train_stem['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9225536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some of the ngrams identified\n",
    "stem_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e965bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode as Unicode\n",
    "stem_feature_names = [l.encode(\"utf-8\") for l in stem_vectorizer.get_feature_names_out()]\n",
    "\n",
    "# Output features to file\n",
    "with open(\"split_1/stem_vectorizer_features.txt\", \"wb\") as f:\n",
    "    for l in stem_feature_names:\n",
    "        f.write(b'%s\\n'%l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try t-test to identify terms that are strongly associated with only positive or only negative reviews.\n",
    "\n",
    "dtm_stem_array = dtm_train_stem.toarray()\n",
    "dtm_stem_pos = dtm_stem_array[train_stem.sentiment == 1, :]\n",
    "dtm_stem_neg = dtm_stem_array[train_stem.sentiment == 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e54411",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_stem_pos_count = dtm_stem_pos.shape[0]\n",
    "dtm_stem_neg_count = dtm_stem_neg.shape[0]\n",
    "dtm_stem_pos_count, dtm_stem_neg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_stem_pos_means = np.mean(dtm_stem_pos, axis=0)\n",
    "dtm_stem_pos_vars = np.var(dtm_stem_pos, axis=0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_stem_neg_means = np.mean(dtm_stem_neg, axis=0)\n",
    "dtm_stem_neg_vars = np.var(dtm_stem_neg, axis=0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8998f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each term / ngram, compute t-statistic for two independent samples.\n",
    "# Hmmm...they're not independent, but we can't really pool the variance...\n",
    "\n",
    "stem_t_statistics = (dtm_stem_pos_means - dtm_stem_neg_means) / np.sqrt((dtm_stem_pos_vars/dtm_stem_pos_count) + (dtm_stem_neg_vars/dtm_stem_neg_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb452544",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_feature_ngrams = stem_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19296a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_feature_statistic_df = pd.DataFrame({\"feature\": stem_feature_ngrams.tolist(), \"statistic\": stem_t_statistics.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many terms meet the 0.05 significance threshold?\n",
    "\n",
    "len(feature_statistic_df[abs(feature_statistic_df.statistic) >= 1.645])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d96629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at top 50 positive ngrams\n",
    "stem_feature_statistic_df.sort_values(by=\"statistic\", ascending=False).iloc[0:50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e602aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at bottom 50 ngrams (most negative)\n",
    "stem_feature_statistic_df.sort_values(by=\"statistic\").iloc[0:50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a20bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the 2000 most predictive tokens based on their t-statistics\n",
    "n_tokens = 2000\n",
    "\n",
    "stem_feature_statistic_df[\"abs_statistic\"] = abs(stem_feature_statistic_df[\"statistic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b94fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_predictive_tokens = stem_feature_statistic_df.sort_values(by=\"abs_statistic\", ascending=False).iloc[:n_tokens, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34151247",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_predictive_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add words that only appeared in positive reviews\n",
    "dtm_stem_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_feature_ngrams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_positive = stem_feature_ngrams[np.logical_and((dtm_stem_pos_means > 0), (dtm_stem_neg_means == 0))]\n",
    "only_negative = stem_feature_ngrams[np.logical_and((dtm_stem_pos_means == 0), (dtm_stem_neg_means > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49629e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a3a61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
