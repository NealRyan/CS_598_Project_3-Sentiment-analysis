{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098d1b18",
   "metadata": {},
   "source": [
    "# CS 598 PSL Project 3: approach based on Campuswire post [628](https://campuswire.com/c/G06C55090/feed/628)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c705d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from nltk import word_tokenize          \n",
    "#from nltk.stem import PorterStemmer\n",
    "#from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "SEED = 4031\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d414006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk import download\n",
    "#download(\"punkt\")\n",
    "#download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54402b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in datasets\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "test_ys = []\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    folder = f\"split_{fold+1}/\"\n",
    "    train_datasets.append(pd.read_csv(folder + \"train.tsv\", sep=\"\\t\"))\n",
    "    test_datasets.append(pd.read_csv(folder + \"test.tsv\", sep=\"\\t\"))\n",
    "    test_ys.append(pd.read_csv(folder + \"test_y.tsv\", sep=\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "443c6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stopword list\n",
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"their\", \"they\", \"his\", \\\n",
    "             \"her\", \"she\", \"he\", \"a\", \"an\", \"and\", \"is\", \"was\", \"are\", \"were\", \"him\", \"himself\", \"has\", \"have\", \"it\", \"its\", \\\n",
    "             \"the\", \"us\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590891c",
   "metadata": {},
   "source": [
    "## Construct vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11e6d8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>naturally in a film who is main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>afraid of the dark left me with the impression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>this has to be one of the biggest misfires eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>this is one of those movies i watched, and won...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>this movie was dreadful. biblically very inacc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124995</th>\n",
       "      <td>0</td>\n",
       "      <td>I am a student of film, and have been for seve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124996</th>\n",
       "      <td>0</td>\n",
       "      <td>It seems like more consideration has gone into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124997</th>\n",
       "      <td>0</td>\n",
       "      <td>I don't believe they made this film. Completel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124998</th>\n",
       "      <td>0</td>\n",
       "      <td>This 30 minute documentary Buñuel made in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124999</th>\n",
       "      <td>1</td>\n",
       "      <td>I saw this movie as a child and it broke my he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                             review\n",
       "0               1  naturally in a film who is main themes are of ...\n",
       "1               0  afraid of the dark left me with the impression...\n",
       "2               0  this has to be one of the biggest misfires eve...\n",
       "3               0  this is one of those movies i watched, and won...\n",
       "4               0  this movie was dreadful. biblically very inacc...\n",
       "...           ...                                                ...\n",
       "124995          0  I am a student of film, and have been for seve...\n",
       "124996          0  It seems like more consideration has gone into...\n",
       "124997          0  I don't believe they made this film. Completel...\n",
       "124998          0  This 30 minute documentary Buñuel made in the ...\n",
       "124999          1  I saw this movie as a child and it broke my he...\n",
       "\n",
       "[125000 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use all training data to construct vocabulary.\n",
    "\n",
    "all_train = pd.concat(train_datasets, axis=0, ignore_index=True)\n",
    "all_train.drop(columns=[\"id\"], inplace=True)\n",
    "all_train\n",
    "\n",
    "#all_train = pd.DataFrame()\n",
    "\n",
    "#for train_df in train_datasets:\n",
    "#    all_train = pd.concat([all_train, train_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a62e0",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "28864621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags and convert to lowercase\n",
    "all_train[\"review\"] = all_train[\"review\"].str.replace('<.*?>', ' ', regex=True)\n",
    "# Convert all strings to lowercase\n",
    "all_train[\"review\"] = all_train[\"review\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d9823e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(reviews):\n",
    "    \n",
    "    \"\"\"\n",
    "    Routine to expand English contractions, like \"isn't\" --> \"is not\".\n",
    "    This is because \"isn't good\" and \"wasn't good\" will both expand to produce the bi-gram \"not good\".\n",
    "    The pooled phrase should have more predictive power than the original two phrases.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary of English contractions. Taken from StackOverflow post, which borrowed it from Wikipedia:\n",
    "    # https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "\n",
    "    contractions = { \n",
    "        \"\\\\bain't\\\\b\": \"am not\",\n",
    "        \"\\\\baren't\\\\b\": \"are not\",\n",
    "        \"\\\\bcan't\\\\b\": \"cannot\",\n",
    "        \"\\\\bcan't've\\\\b\": \"cannot have\",\n",
    "        \"\\\\b'cause\\\\b\": \"because\",\n",
    "        \"\\\\bcould've\\\\b\": \"could have\",\n",
    "        \"\\\\bcouldn't\\\\b\": \"could not\",\n",
    "        \"\\\\bcouldn't've\\\\b\": \"could not have\",\n",
    "        \"\\\\bdidn't\\\\b\": \"did not\",\n",
    "        \"\\\\bdoesn't\\\\b\": \"does not\",\n",
    "        \"\\\\bdon't\\\\b\": \"do not\",\n",
    "        \"\\\\bhadn't\\\\b\": \"had not\",\n",
    "        \"\\\\bhadn't've\\\\b\": \"had not have\",\n",
    "        \"\\\\bhasn't\\\\b\": \"has not\",\n",
    "        \"\\\\bhaven't\\\\b\": \"have not\",\n",
    "        \"\\\\bhe'd\\\\b\": \"he would\",\n",
    "        \"\\\\bhe'd've\\\\b\": \"he would have\",\n",
    "        \"\\\\bhe'll\\\\b\": \"he will\",\n",
    "        \"\\\\bhe'll've\\\\b\": \"he will have\",\n",
    "        \"\\\\bhe's\\\\b\": \"he is\",\n",
    "        \"\\\\bhow'd\\\\b\": \"how did\",\n",
    "        \"\\\\bhow'd'y\\\\b\": \"how do you\",\n",
    "        \"\\\\bhow'll\\\\b\": \"how will\",\n",
    "        \"\\\\bhow's\\\\b\": \"how is\",\n",
    "        \"\\\\bi'd\\\\b\": \"i would\",\n",
    "        \"\\\\bi'd've\\\\b\": \"i would have\",\n",
    "        \"\\\\bi'll\\\\b\": \"i will\",\n",
    "        \"\\\\bi'll've\\\\b\": \"i will have\",\n",
    "        \"\\\\bi'm\\\\b\": \"i am\",\n",
    "        \"\\\\bi've\\\\b\": \"i have\",\n",
    "        \"\\\\bisn't\\\\b\": \"is not\",\n",
    "        \"\\\\bit'd\\\\b\": \"it would\",\n",
    "        \"\\\\bit'd've\\\\b\": \"it would have\",\n",
    "        \"\\\\bit'll\\\\b\": \"it will\",\n",
    "        \"\\\\bit'll've\\\\b\": \"it will have\",\n",
    "        \"\\\\bit's\\\\b\": \"it is\",\n",
    "        \"\\\\blet's\\\\b\": \"let us\",\n",
    "        \"\\\\bma'am\\\\b\": \"madam\",\n",
    "        \"\\\\bmayn't\\\\b\": \"may not\",\n",
    "        \"\\\\bmight've\\\\b\": \"might have\",\n",
    "        \"\\\\bmightn't\\\\b\": \"might not\",\n",
    "        \"\\\\bmightn't've\\\\b\": \"might not have\",\n",
    "        \"\\\\bmust've\\\\b\": \"must have\",\n",
    "        \"\\\\bmustn't\\\\b\": \"must not\",\n",
    "        \"\\\\bmustn't've\\\\b\": \"must not have\",\n",
    "        \"\\\\bneedn't\\\\b\": \"need not\",\n",
    "        \"\\\\bneedn't've\\\\b\": \"need not have\",\n",
    "        \"\\\\bo'clock\\\\b\": \"of the clock\",\n",
    "        \"\\\\boughtn't\\\\b\": \"ought not\",\n",
    "        \"\\\\boughtn't've\\\\b\": \"ought not have\",\n",
    "        \"\\\\bshan't\\\\b\": \"shall not\",\n",
    "        \"\\\\bsha'n't\\\\b\": \"shall not\",\n",
    "        \"\\\\bshan't've\\\\b\": \"shall not have\",\n",
    "        \"\\\\bshe'd\\\\b\": \"she would\",\n",
    "        \"\\\\bshe'd've\\\\b\": \"she would have\",\n",
    "        \"\\\\bshe'll\\\\b\": \"she will\",\n",
    "        \"\\\\bshe'll've\\\\b\": \"she will have\",\n",
    "        \"\\\\bshe's\\\\b\": \"she is\",\n",
    "        \"\\\\bshould've\\\\b\": \"should have\",\n",
    "        \"\\\\bshouldn't\\\\b\": \"should not\",\n",
    "        \"\\\\bshouldn't've\\\\b\": \"should not have\",\n",
    "        \"\\\\bso've\\\\b\": \"so have\",\n",
    "        \"\\\\bso's\\\\b\": \"so is\",\n",
    "        \"\\\\bthat'd\\\\b\": \"that would\",\n",
    "        \"\\\\bthat'd've\\\\b\": \"that would have\",\n",
    "        \"\\\\bthat's\\\\b\": \"that is\",\n",
    "        \"\\\\bthere'd\\\\b\": \"there would\",\n",
    "        \"\\\\bthere'd've\\\\b\": \"there would have\",\n",
    "        \"\\\\bthere's\\\\b\": \"there is\",\n",
    "        \"\\\\bthey'd\\\\b\": \"they would\",\n",
    "        \"\\\\bthey'd've\\\\b\": \"they would have\",\n",
    "        \"\\\\bthey'll\\\\b\": \"they will\",\n",
    "        \"\\\\bthey'll've\\\\b\": \"they will have\",\n",
    "        \"\\\\bthey're\\\\b\": \"they are\",\n",
    "        \"\\\\bthey've\\\\b\": \"they have\",\n",
    "        \"\\\\bto've\\\\b\": \"to have\",\n",
    "        \"\\\\bwasn't\\\\b\": \"was not\",\n",
    "        \"\\\\bwe'd\\\\b\": \"we would\",\n",
    "        \"\\\\bwe'd've\\\\b\": \"we would have\",\n",
    "        \"\\\\bwe'll\\\\b\": \"we will\",\n",
    "        \"\\\\bwe'll've\\\\b\": \"we will have\",\n",
    "        \"\\\\bwe're\\\\b\": \"we are\",\n",
    "        \"\\\\bwe've\\\\b\": \"we have\",\n",
    "        \"\\\\bweren't\\\\b\": \"were not\",\n",
    "        \"\\\\bwhat'll\\\\b\": \"what will\",\n",
    "        \"\\\\bwhat'll've\\\\b\": \"what will have\",\n",
    "        \"\\\\bwhat're\\\\b\": \"what are\",\n",
    "        \"\\\\bwhat's\\\\b\": \"what is\",\n",
    "        \"\\\\bwhat've\\\\b\": \"what have\",\n",
    "        \"\\\\bwhen's\\\\b\": \"when is\",\n",
    "        \"\\\\bwhen've\\\\b\": \"when have\",\n",
    "        \"\\\\bwhere'd\\\\b\": \"where did\",\n",
    "        \"\\\\bwhere's\\\\b\": \"where is\",\n",
    "        \"\\\\bwhere've\\\\b\": \"where have\",\n",
    "        \"\\\\bwho'll\\\\b\": \"who will\",\n",
    "        \"\\\\bwho'll've\\\\b\": \"who will have\",\n",
    "        \"\\\\bwho's\\\\b\": \"who is\",\n",
    "        \"\\\\bwho've\\\\b\": \"who have\",\n",
    "        \"\\\\bwhy's\\\\b\": \"why is\",\n",
    "        \"\\\\bwhy've\\\\b\": \"why have\",\n",
    "        \"\\\\bwill've\\\\b\": \"will have\",\n",
    "        \"\\\\bwon't\\\\b\": \"will not\",\n",
    "        \"\\\\bwon't've\\\\b\": \"will not have\",\n",
    "        \"\\\\bwould've\\\\b\": \"would have\",\n",
    "        \"\\\\bwouldn't\\\\b\": \"would not\",\n",
    "        \"\\\\bwouldn't've\\\\b\": \"would not have\",\n",
    "        \"\\\\by'all\\\\b\": \"you all\",\n",
    "        \"\\\\by'all'd\\\\b\": \"you all would\",\n",
    "        \"\\\\by'all'd've\\\\b\": \"you all would have\",\n",
    "        \"\\\\by'all're\\\\b\": \"you all are\",\n",
    "        \"\\\\by'all've\\\\b\": \"you all have\",\n",
    "        \"\\\\byou'd\\\\b\": \"you would\",\n",
    "        \"\\\\byou'd've\\\\b\": \"you would have\",\n",
    "        \"\\\\byou'll\\\\b\": \"you will\",\n",
    "        \"\\\\byou'll've\\\\b\": \"you will have\",\n",
    "        \"\\\\byou're\\\\b\": \"you are\",\n",
    "        \"\\\\byou've\\\\b\": \"you have\"\n",
    "    }\n",
    "    \n",
    "    # Replace all contractions in all reviews.\n",
    "    for contraction in contractions:\n",
    "        reviews = reviews.str.replace(contraction, contractions[contraction], regex=True)\n",
    "        \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ffa4b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand English contractions\n",
    "all_train[\"review\"] = expand_contractions(all_train[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1edfb8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the reviews\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    #preprocessor=lambda x: x.lower(), # Convert to lowercase\n",
    "    stop_words=stopwords,             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                     # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r\"\\b[\\w+\\|']+\\b\"    # Use word tokenizer, but don't split on apostrophes\n",
    ")\n",
    "\n",
    "dtm_train = vectorizer.fit_transform(all_train[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "747cc604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31701,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the number of ngrams\n",
    "feature_ngrams = vectorizer.get_feature_names_out()\n",
    "feature_ngrams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5b19b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output features to file\n",
    "np.savetxt(\"all_train_features.txt\", feature_ngrams, fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e1460b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to preprocess text: strip out HTML, convert to lowercase, and expand English contractions.\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = reviews.str.replace('<.*?>', ' ', regex=True)\n",
    "    reviews = reviews.str.lower()\n",
    "    reviews = expand_contractions(reviews)\n",
    "    \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2f9d9",
   "metadata": {},
   "source": [
    "### Use t-test to identify strongest 2000 positive and negative terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0d579ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try t-test to identify terms that are strongly associated with only positive or only negative reviews.\n",
    "\n",
    "dtm_array = dtm_train.toarray()\n",
    "dtm_pos = dtm_array[all_train.sentiment == 1, :]\n",
    "dtm_neg = dtm_array[all_train.sentiment == 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3102a72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62385, 62615)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_pos_count = dtm_pos.shape[0]\n",
    "dtm_neg_count = dtm_neg.shape[0]\n",
    "dtm_pos_count, dtm_neg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "da0ae763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31701,)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_pos_means = np.mean(dtm_pos, axis=0)\n",
    "dtm_pos_vars = np.var(dtm_pos, axis=0, ddof=1)\n",
    "dtm_pos_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2534456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_neg_means = np.mean(dtm_neg, axis=0)\n",
    "dtm_neg_vars = np.var(dtm_neg, axis=0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1c65289b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31701,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each term / ngram, compute t-statistic for two independent samples.\n",
    "# Hmmm...they're not independent, but we can't really pool the variance...\n",
    "\n",
    "t_statistics = (dtm_pos_means - dtm_neg_means) / np.sqrt((dtm_pos_vars/dtm_pos_count) + (dtm_neg_vars/dtm_neg_count))\n",
    "t_statistics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1bef114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_statistic_df = pd.DataFrame({\"feature\": feature_ngrams.tolist(), \"statistic\": t_statistics.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "52a01f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>great</td>\n",
       "      <td>73.982716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8232</th>\n",
       "      <td>excellent</td>\n",
       "      <td>58.862798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31186</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>53.246837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>best</td>\n",
       "      <td>53.155283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17806</th>\n",
       "      <td>of best</td>\n",
       "      <td>51.324386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19152</th>\n",
       "      <td>one of best</td>\n",
       "      <td>48.932390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14764</th>\n",
       "      <td>love</td>\n",
       "      <td>43.829750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20103</th>\n",
       "      <td>perfect</td>\n",
       "      <td>41.751995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>amazing</td>\n",
       "      <td>40.271278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>39.589658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14801</th>\n",
       "      <td>loved</td>\n",
       "      <td>39.514111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29870</th>\n",
       "      <td>well</td>\n",
       "      <td>38.451604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24828</th>\n",
       "      <td>superb</td>\n",
       "      <td>38.215725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>brilliant</td>\n",
       "      <td>38.141745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8679</th>\n",
       "      <td>favorite</td>\n",
       "      <td>37.800354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14257</th>\n",
       "      <td>life</td>\n",
       "      <td>36.662636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>highly</td>\n",
       "      <td>35.418083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16536</th>\n",
       "      <td>must see</td>\n",
       "      <td>35.275773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>also</td>\n",
       "      <td>34.927260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29228</th>\n",
       "      <td>very</td>\n",
       "      <td>33.866042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19149</th>\n",
       "      <td>one of</td>\n",
       "      <td>33.427117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8586</th>\n",
       "      <td>fantastic</td>\n",
       "      <td>33.146335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>beautifully</td>\n",
       "      <td>33.106315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20115</th>\n",
       "      <td>performance</td>\n",
       "      <td>33.104688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3618</th>\n",
       "      <td>both</td>\n",
       "      <td>32.063273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29343</th>\n",
       "      <td>very well</td>\n",
       "      <td>31.721133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28321</th>\n",
       "      <td>today</td>\n",
       "      <td>31.350023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7804</th>\n",
       "      <td>enjoyed</td>\n",
       "      <td>30.820844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20128</th>\n",
       "      <td>performances</td>\n",
       "      <td>30.688425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>always</td>\n",
       "      <td>30.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29947</th>\n",
       "      <td>well worth</td>\n",
       "      <td>30.359774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>8 10</td>\n",
       "      <td>29.618769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31596</th>\n",
       "      <td>years</td>\n",
       "      <td>29.375083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19721</th>\n",
       "      <td>outstanding</td>\n",
       "      <td>29.215841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11542</th>\n",
       "      <td>highly recommend</td>\n",
       "      <td>29.086497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26557</th>\n",
       "      <td>this great</td>\n",
       "      <td>29.061440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28487</th>\n",
       "      <td>touching</td>\n",
       "      <td>28.927331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10 10</td>\n",
       "      <td>28.664055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31277</th>\n",
       "      <td>world</td>\n",
       "      <td>28.650884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31652</th>\n",
       "      <td>young</td>\n",
       "      <td>28.566383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24610</th>\n",
       "      <td>strong</td>\n",
       "      <td>28.536829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20657</th>\n",
       "      <td>powerful</td>\n",
       "      <td>28.514348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31194</th>\n",
       "      <td>wonderfully</td>\n",
       "      <td>28.452246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29877</th>\n",
       "      <td>well as</td>\n",
       "      <td>28.417276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>definitely</td>\n",
       "      <td>28.268586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20112</th>\n",
       "      <td>perfectly</td>\n",
       "      <td>28.164201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>as well</td>\n",
       "      <td>28.115403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11546</th>\n",
       "      <td>highly recommended</td>\n",
       "      <td>27.925894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>7 10</td>\n",
       "      <td>27.507769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>as well as</td>\n",
       "      <td>27.434764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature  statistic\n",
       "10874               great  73.982716\n",
       "8232            excellent  58.862798\n",
       "31186           wonderful  53.246837\n",
       "3247                 best  53.155283\n",
       "17806             of best  51.324386\n",
       "19152         one of best  48.932390\n",
       "14764                love  43.829750\n",
       "20103             perfect  41.751995\n",
       "1406              amazing  40.271278\n",
       "2936            beautiful  39.589658\n",
       "14801               loved  39.514111\n",
       "29870                well  38.451604\n",
       "24828              superb  38.215725\n",
       "3739            brilliant  38.141745\n",
       "8679             favorite  37.800354\n",
       "14257                life  36.662636\n",
       "11540              highly  35.418083\n",
       "16536            must see  35.275773\n",
       "1224                 also  34.927260\n",
       "29228                very  33.866042\n",
       "19149              one of  33.427117\n",
       "8586            fantastic  33.146335\n",
       "2949          beautifully  33.106315\n",
       "20115         performance  33.104688\n",
       "3618                 both  32.063273\n",
       "29343           very well  31.721133\n",
       "28321               today  31.350023\n",
       "7804              enjoyed  30.820844\n",
       "20128        performances  30.688425\n",
       "1307               always  30.646900\n",
       "29947          well worth  30.359774\n",
       "301                  8 10  29.618769\n",
       "31596               years  29.375083\n",
       "19721         outstanding  29.215841\n",
       "11542    highly recommend  29.086497\n",
       "26557          this great  29.061440\n",
       "28487            touching  28.927331\n",
       "24                  10 10  28.664055\n",
       "31277               world  28.650884\n",
       "31652               young  28.566383\n",
       "24610              strong  28.536829\n",
       "20657            powerful  28.514348\n",
       "31194         wonderfully  28.452246\n",
       "29877             well as  28.417276\n",
       "6334           definitely  28.268586\n",
       "20112           perfectly  28.164201\n",
       "2075              as well  28.115403\n",
       "11546  highly recommended  27.925894\n",
       "289                  7 10  27.507769\n",
       "2076           as well as  27.434764"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at top 50 positive words\n",
    "feature_statistic_df.sort_values(by=\"statistic\", ascending=False).iloc[0:50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "65beb5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>statistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>bad</td>\n",
       "      <td>-94.755462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31315</th>\n",
       "      <td>worst</td>\n",
       "      <td>-84.764359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29641</th>\n",
       "      <td>waste</td>\n",
       "      <td>-68.997657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>awful</td>\n",
       "      <td>-64.850629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17113</th>\n",
       "      <td>not even</td>\n",
       "      <td>-58.121304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25264</th>\n",
       "      <td>terrible</td>\n",
       "      <td>-57.600314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31306</th>\n",
       "      <td>worse</td>\n",
       "      <td>-53.553785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>boring</td>\n",
       "      <td>-52.661545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24670</th>\n",
       "      <td>stupid</td>\n",
       "      <td>-50.814659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16835</th>\n",
       "      <td>no</td>\n",
       "      <td>-50.695585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17540</th>\n",
       "      <td>nothing</td>\n",
       "      <td>-50.552694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29644</th>\n",
       "      <td>waste of</td>\n",
       "      <td>-50.402360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18668</th>\n",
       "      <td>of worst</td>\n",
       "      <td>-49.201362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11716</th>\n",
       "      <td>horrible</td>\n",
       "      <td>-48.510445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20542</th>\n",
       "      <td>poor</td>\n",
       "      <td>-48.450687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15622</th>\n",
       "      <td>minutes</td>\n",
       "      <td>-47.237653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>even</td>\n",
       "      <td>-45.669468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13504</th>\n",
       "      <td>just</td>\n",
       "      <td>-45.419072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23640</th>\n",
       "      <td>so bad</td>\n",
       "      <td>-44.844958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5936</th>\n",
       "      <td>crap</td>\n",
       "      <td>-44.480812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19199</th>\n",
       "      <td>one of worst</td>\n",
       "      <td>-43.562671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20548</th>\n",
       "      <td>poorly</td>\n",
       "      <td>-43.186878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24867</th>\n",
       "      <td>supposed</td>\n",
       "      <td>-43.143946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>acting</td>\n",
       "      <td>-43.045135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>at all</td>\n",
       "      <td>-42.293576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24868</th>\n",
       "      <td>supposed to</td>\n",
       "      <td>-41.246715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21954</th>\n",
       "      <td>ridiculous</td>\n",
       "      <td>-41.040249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20425</th>\n",
       "      <td>plot</td>\n",
       "      <td>-40.879246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5783</th>\n",
       "      <td>could</td>\n",
       "      <td>-40.671791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30629</th>\n",
       "      <td>why</td>\n",
       "      <td>-40.617536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6864</th>\n",
       "      <td>do</td>\n",
       "      <td>-40.385450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22627</th>\n",
       "      <td>script</td>\n",
       "      <td>-40.084532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6898</th>\n",
       "      <td>do not</td>\n",
       "      <td>-39.541573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13944</th>\n",
       "      <td>lame</td>\n",
       "      <td>-39.516830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29647</th>\n",
       "      <td>waste of time</td>\n",
       "      <td>-39.067059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2422</th>\n",
       "      <td>avoid</td>\n",
       "      <td>-38.959725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31328</th>\n",
       "      <td>worst movie</td>\n",
       "      <td>-38.241648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29657</th>\n",
       "      <td>wasted</td>\n",
       "      <td>-37.762129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17497</th>\n",
       "      <td>not waste</td>\n",
       "      <td>-37.274232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>annoying</td>\n",
       "      <td>-37.113367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7359</th>\n",
       "      <td>dull</td>\n",
       "      <td>-36.605027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29649</th>\n",
       "      <td>waste time</td>\n",
       "      <td>-36.465669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6982</th>\n",
       "      <td>do not waste</td>\n",
       "      <td>-35.880702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18746</th>\n",
       "      <td>oh</td>\n",
       "      <td>-35.761045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20501</th>\n",
       "      <td>pointless</td>\n",
       "      <td>-35.559514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15740</th>\n",
       "      <td>money</td>\n",
       "      <td>-35.548515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15481</th>\n",
       "      <td>mess</td>\n",
       "      <td>-35.506881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4943</th>\n",
       "      <td>cheap</td>\n",
       "      <td>-35.372461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14033</th>\n",
       "      <td>laughable</td>\n",
       "      <td>-34.981153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24869</th>\n",
       "      <td>supposed to be</td>\n",
       "      <td>-34.978736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature  statistic\n",
       "2537              bad -94.755462\n",
       "31315           worst -84.764359\n",
       "29641           waste -68.997657\n",
       "2465            awful -64.850629\n",
       "17113        not even -58.121304\n",
       "25264        terrible -57.600314\n",
       "31306           worse -53.553785\n",
       "3598           boring -52.661545\n",
       "24670          stupid -50.814659\n",
       "16835              no -50.695585\n",
       "17540         nothing -50.552694\n",
       "29644        waste of -50.402360\n",
       "18668        of worst -49.201362\n",
       "11716        horrible -48.510445\n",
       "20542            poor -48.450687\n",
       "15622         minutes -47.237653\n",
       "7992             even -45.669468\n",
       "13504            just -45.419072\n",
       "23640          so bad -44.844958\n",
       "5936             crap -44.480812\n",
       "19199    one of worst -43.562671\n",
       "20548          poorly -43.186878\n",
       "24867        supposed -43.143946\n",
       "567            acting -43.045135\n",
       "2169           at all -42.293576\n",
       "24868     supposed to -41.246715\n",
       "21954      ridiculous -41.040249\n",
       "20425            plot -40.879246\n",
       "5783            could -40.671791\n",
       "30629             why -40.617536\n",
       "6864               do -40.385450\n",
       "22627          script -40.084532\n",
       "6898           do not -39.541573\n",
       "13944            lame -39.516830\n",
       "29647   waste of time -39.067059\n",
       "2422            avoid -38.959725\n",
       "31328     worst movie -38.241648\n",
       "29657          wasted -37.762129\n",
       "17497       not waste -37.274232\n",
       "1510         annoying -37.113367\n",
       "7359             dull -36.605027\n",
       "29649      waste time -36.465669\n",
       "6982     do not waste -35.880702\n",
       "18746              oh -35.761045\n",
       "20501       pointless -35.559514\n",
       "15740           money -35.548515\n",
       "15481            mess -35.506881\n",
       "4943            cheap -35.372461\n",
       "14033       laughable -34.981153\n",
       "24869  supposed to be -34.978736"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at bottom 50 words (most negative)\n",
    "feature_statistic_df.sort_values(by=\"statistic\").iloc[0:50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0a04c61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11752"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many terms meet the 0.05 significance threshold?\n",
    "len(feature_statistic_df[feature_statistic_df.statistic >= 1.645])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c3e090ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           review\n",
       "sentiment        \n",
       "0           62615\n",
       "1           62385"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for class imbalance\n",
    "all_train.groupby([\"sentiment\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "93fd2880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2537                     bad\n",
       "31315                  worst\n",
       "10874                  great\n",
       "29641                  waste\n",
       "2465                   awful\n",
       "                ...         \n",
       "24033                sounded\n",
       "16860              no excuse\n",
       "26662    this movie horrible\n",
       "24173                spot on\n",
       "17214            not in good\n",
       "Name: feature, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep the top 2000 terms by magnitude of t-statistic\n",
    "\n",
    "n_terms = 2000\n",
    "\n",
    "feature_statistic_df[\"abs_statistic\"] = abs(feature_statistic_df[\"statistic\"])\n",
    "\n",
    "top_features = feature_statistic_df.sort_values(by=\"abs_statistic\", ascending=False).iloc[:n_terms, 0]\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d1229ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['avoid like', 'bad as this', 'bad music', 'could not save',\n",
       "       'crap like this', 'do not waste money', 'easily worst', 'gave 2',\n",
       "       'how not to make', 'instead of 1', 'manos', 'not funny not',\n",
       "       'not waste money on', 'not waste time or', 'this by far worst',\n",
       "       'this drivel', 'this dull', 'this junk', 'this lame',\n",
       "       'this piece of garbage', 'this rubbish', 'this stinker',\n",
       "       'this tripe', 'this turkey', 'this waste', 'this waste of',\n",
       "       'waste time or', 'waste time or money', 'waste time with this',\n",
       "       'worst movie ever made', 'worst movies ever seen'], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add any terms that only appeared in positive reviews or only in negative reviews.\n",
    "only_positive = feature_ngrams[np.logical_and((dtm_pos_means > 0), (dtm_neg_means == 0))]\n",
    "only_negative = feature_ngrams[np.logical_and((dtm_pos_means == 0), (dtm_neg_means > 0))]\n",
    "only_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c6137619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['really wanted',\n",
       " 'not to be missed',\n",
       " 'as good',\n",
       " 'rest of',\n",
       " 'heartbreaking',\n",
       " 'wooden',\n",
       " 'credibility',\n",
       " 'stay away from',\n",
       " 'f',\n",
       " 'if not seen',\n",
       " 'performances of',\n",
       " 'this crap',\n",
       " 'sucks',\n",
       " 'watch again',\n",
       " 'waste of film',\n",
       " 'haunting',\n",
       " 'tried',\n",
       " 'save yourself',\n",
       " 'horror movie',\n",
       " 'this movie bad',\n",
       " 'meaningless',\n",
       " 'cardboard',\n",
       " 'most annoying',\n",
       " 'funny not',\n",
       " 'tells',\n",
       " 'turkey',\n",
       " 'modern',\n",
       " 'than this',\n",
       " 'this rubbish',\n",
       " 'loves',\n",
       " 'cash in on',\n",
       " 'no excuse',\n",
       " 'to live',\n",
       " 'hated',\n",
       " 'plan 9 from',\n",
       " 'may not be',\n",
       " 'in first place',\n",
       " 'supporting cast',\n",
       " 'keeps',\n",
       " 'awfulness',\n",
       " 'based on true story',\n",
       " 'killed',\n",
       " 'musical',\n",
       " 'nothing to',\n",
       " 'for those',\n",
       " 'to sit through',\n",
       " 'to recommend',\n",
       " 'one of all',\n",
       " 'surprised',\n",
       " 'obvious',\n",
       " 'non existent',\n",
       " 'bad good',\n",
       " 'far worst',\n",
       " 'portrayal of',\n",
       " 'tale',\n",
       " 'john',\n",
       " 'only good thing about',\n",
       " 'tense',\n",
       " 'bad if',\n",
       " 'hour',\n",
       " 'to be missed',\n",
       " 'only redeeming',\n",
       " 'excuse for',\n",
       " 'how not to',\n",
       " 'funny at all',\n",
       " 'one of best of',\n",
       " '1 out of 10',\n",
       " 'would not recommend',\n",
       " 'sorry to',\n",
       " 'when first',\n",
       " 'warn',\n",
       " 'theme',\n",
       " '10 10',\n",
       " 'famous',\n",
       " 'redeeming',\n",
       " 'still great',\n",
       " 'writers',\n",
       " 'travesty',\n",
       " 'spectacular',\n",
       " 'superb',\n",
       " 'to make',\n",
       " 'minutes of',\n",
       " 'disaster',\n",
       " 'fails on',\n",
       " 'begin with',\n",
       " 'mean',\n",
       " 'in new',\n",
       " 'just too',\n",
       " 'not work',\n",
       " 'frank',\n",
       " 'in many ways',\n",
       " 'even get',\n",
       " 'this bad',\n",
       " 'also excellent',\n",
       " 'insult',\n",
       " 'not much',\n",
       " 'mst3k',\n",
       " 'this stupid',\n",
       " \"today's\",\n",
       " 'played by',\n",
       " 'this best',\n",
       " 'of all time',\n",
       " 'maybe',\n",
       " 'of best of',\n",
       " 'premise',\n",
       " 'to act',\n",
       " 'would been',\n",
       " 'definitely',\n",
       " 'flick',\n",
       " 'perfect in',\n",
       " 'incoherent',\n",
       " 'might',\n",
       " 'that this',\n",
       " 'zero',\n",
       " 'available',\n",
       " 'at all',\n",
       " 'gives',\n",
       " 'cannot even',\n",
       " 'sense of',\n",
       " 'creates',\n",
       " 'to laugh',\n",
       " 'reality',\n",
       " 'this pile of',\n",
       " 'young man',\n",
       " 'avoid this movie',\n",
       " 'there no plot',\n",
       " 'no sense at',\n",
       " 'very touching',\n",
       " 'cannot',\n",
       " 'attempt at',\n",
       " 'gem',\n",
       " 'very boring',\n",
       " 'not waste money on',\n",
       " 'this one of worst',\n",
       " 'complete waste',\n",
       " 'for everyone',\n",
       " 'money to',\n",
       " 'plan 9 from outer',\n",
       " 'abomination',\n",
       " 'beauty',\n",
       " 'sat',\n",
       " 'nevertheless',\n",
       " 'give 7',\n",
       " '2 out',\n",
       " 'bad music',\n",
       " 'problem with',\n",
       " 'save money',\n",
       " 'tries to',\n",
       " 'good thing',\n",
       " 'lousy',\n",
       " 'biggest problem',\n",
       " 'truly awful',\n",
       " 'sex',\n",
       " 'through this',\n",
       " 'mystery science',\n",
       " 'drama',\n",
       " '9',\n",
       " 'favorite movies',\n",
       " 'should be ashamed',\n",
       " 'boredom',\n",
       " 'not rent',\n",
       " 'uninteresting',\n",
       " 'movie awful',\n",
       " 'failed',\n",
       " 'only thing',\n",
       " 'much better',\n",
       " 'do',\n",
       " 'relate',\n",
       " 'idiot',\n",
       " 'at least',\n",
       " '1 10',\n",
       " 'kills',\n",
       " 'sucked',\n",
       " 'best movie',\n",
       " 'shines',\n",
       " 'instead of 1',\n",
       " 'miserably',\n",
       " 'french',\n",
       " 'film so bad',\n",
       " 'just did not',\n",
       " 'love for',\n",
       " 'lacking',\n",
       " 'really enjoyed',\n",
       " 'married',\n",
       " 'even that',\n",
       " 'will always',\n",
       " 'very believable',\n",
       " 'quite',\n",
       " '20 minutes',\n",
       " 'father',\n",
       " 'saving grace',\n",
       " 'movie could been',\n",
       " 'straight to',\n",
       " 'name',\n",
       " 'any of',\n",
       " 'makes no',\n",
       " 'not care',\n",
       " 'or money',\n",
       " 'no sense',\n",
       " 'wasting time',\n",
       " 'goes nowhere',\n",
       " 'skip',\n",
       " 'looks like',\n",
       " 'lacks',\n",
       " 'sees',\n",
       " 'portrayal',\n",
       " '8 out of 10',\n",
       " 'very different',\n",
       " 'bad that',\n",
       " 'worst movie seen',\n",
       " 'ever',\n",
       " 'to cash',\n",
       " 'but this',\n",
       " 'want',\n",
       " 'twists',\n",
       " 'amateurish',\n",
       " 'chance to',\n",
       " '8',\n",
       " 'brought',\n",
       " 'sensitive',\n",
       " 'not waste money',\n",
       " 'tasteless',\n",
       " '7 out of',\n",
       " 'remotely',\n",
       " 'captures',\n",
       " 'realistic',\n",
       " 'whatever',\n",
       " 'rest',\n",
       " 'bad script',\n",
       " 'worst ever seen',\n",
       " 'like watching',\n",
       " 'repetitive',\n",
       " 'nothing',\n",
       " 'unique',\n",
       " 'not recommend this',\n",
       " 'touch',\n",
       " 'inaccurate',\n",
       " 'imdb',\n",
       " 'personal',\n",
       " 'films',\n",
       " 'some kind',\n",
       " 'little to',\n",
       " 'bored',\n",
       " 'do not care',\n",
       " 'around in',\n",
       " 'sharp',\n",
       " 'gore',\n",
       " 'recommend',\n",
       " 'great as',\n",
       " 'honestly',\n",
       " 'chance',\n",
       " 'on edge',\n",
       " 'clichés',\n",
       " 'cliché',\n",
       " 'but great',\n",
       " 'unconvincing',\n",
       " 'of own',\n",
       " 'to warn',\n",
       " 'beautifully',\n",
       " 'below',\n",
       " 'meets',\n",
       " 'dreck',\n",
       " 'movie do',\n",
       " 'sit',\n",
       " 'horrid',\n",
       " 'some guy',\n",
       " 'whole thing',\n",
       " 'exceptional',\n",
       " 'funniest',\n",
       " 'worth seeing',\n",
       " 'other than',\n",
       " 'would been better',\n",
       " 'more like',\n",
       " 'waste',\n",
       " 'no sense at all',\n",
       " 'unbelievable',\n",
       " 'poor',\n",
       " 'half way through',\n",
       " 'should',\n",
       " 'this waste',\n",
       " 'that bad',\n",
       " 'spot on',\n",
       " 'pretty',\n",
       " 'only good',\n",
       " 'hackneyed',\n",
       " 'always',\n",
       " 'redeeming quality',\n",
       " 'nothing else',\n",
       " 'unless want to',\n",
       " 'then there',\n",
       " '7 out of 10',\n",
       " 'this turkey',\n",
       " 'gritty',\n",
       " 'for worst',\n",
       " 'even worse',\n",
       " 'problem',\n",
       " 'in role',\n",
       " 'poorly made',\n",
       " 'love this film',\n",
       " 'what waste of',\n",
       " 'acting superb',\n",
       " 'do not rent',\n",
       " 'to be comedy',\n",
       " 'steals show',\n",
       " 'along with',\n",
       " 'movie so bad',\n",
       " 'of movie',\n",
       " 'cannot believe',\n",
       " 'muddled',\n",
       " 'shallow',\n",
       " 'why',\n",
       " 'adventure',\n",
       " 'just does',\n",
       " 'incredible',\n",
       " 'inspiring',\n",
       " 'wrong',\n",
       " 'for anyone who',\n",
       " 'worst acting',\n",
       " 'paint dry',\n",
       " 'of 1',\n",
       " 'paid',\n",
       " 'just do not',\n",
       " 'minutes',\n",
       " 'utterly',\n",
       " 'only positive',\n",
       " 'do not miss',\n",
       " 'thing about',\n",
       " 'not watch',\n",
       " 'age',\n",
       " 'light',\n",
       " 'unexpected',\n",
       " 'wasted in',\n",
       " 'please do not',\n",
       " 'wanted to',\n",
       " 'generic',\n",
       " 'well acted',\n",
       " 'in movie',\n",
       " 'flat',\n",
       " 'unlike',\n",
       " 'well done',\n",
       " 'episode',\n",
       " 'available on dvd',\n",
       " 'saw this',\n",
       " 'edge',\n",
       " 'engrossing',\n",
       " 'yourself favor',\n",
       " 'about this',\n",
       " 'achievement',\n",
       " 'believe that',\n",
       " 'first of all',\n",
       " 'crappy',\n",
       " 'if really',\n",
       " 'painful',\n",
       " 'awesome',\n",
       " 'excellent performances',\n",
       " 'edge of',\n",
       " 'sitting through',\n",
       " 'tripe',\n",
       " 'waste of',\n",
       " 'strongly',\n",
       " 'no redeeming',\n",
       " 'dimensional',\n",
       " 'uninspired',\n",
       " 'affection',\n",
       " 'halfway',\n",
       " 'totally',\n",
       " 'painful to watch',\n",
       " 'do with',\n",
       " 'times',\n",
       " 'definitely recommend',\n",
       " 'everyone should',\n",
       " 'for no reason',\n",
       " 'watching this',\n",
       " 'for free',\n",
       " 'with excellent',\n",
       " 'simplicity',\n",
       " 'does nothing',\n",
       " 'not funny',\n",
       " 'cannot wait',\n",
       " 'not seen',\n",
       " 'bad',\n",
       " 'avoid at all costs',\n",
       " 'for no apparent',\n",
       " 'no character',\n",
       " 'writing',\n",
       " 'renting',\n",
       " 'be ashamed',\n",
       " 'existent',\n",
       " 'just great',\n",
       " 'not recommend',\n",
       " 'no',\n",
       " 'among',\n",
       " 'movie 1',\n",
       " 'unwatchable',\n",
       " 'illogical',\n",
       " 'cannot act',\n",
       " 'images',\n",
       " 'others',\n",
       " 'worst films ever seen',\n",
       " 'with this',\n",
       " 'of worst',\n",
       " 'worse',\n",
       " 'blood',\n",
       " 'excellently',\n",
       " 'highly',\n",
       " 'on edge of',\n",
       " 'of worst movies',\n",
       " 'had',\n",
       " 'best of',\n",
       " 'love',\n",
       " 'do not bother with',\n",
       " 'stewart',\n",
       " 'miss this',\n",
       " 'but this just',\n",
       " 'no wonder',\n",
       " 'entire',\n",
       " 'could been so much',\n",
       " 'movie there',\n",
       " 'excellent film',\n",
       " 'believe',\n",
       " '9 out',\n",
       " 'disappointing',\n",
       " 'rest of movie',\n",
       " 'or even',\n",
       " 'thin',\n",
       " 'carries',\n",
       " 'love story',\n",
       " 'film noir',\n",
       " '4 10',\n",
       " 'impressive',\n",
       " 'awful acting',\n",
       " 'of war',\n",
       " 'no chemistry',\n",
       " 'pathetic',\n",
       " 'advice',\n",
       " 'cried',\n",
       " 'suck',\n",
       " 'anything',\n",
       " 'ugh',\n",
       " 'today',\n",
       " 'not perfect',\n",
       " 'not waste time with',\n",
       " 'where to',\n",
       " 'but still',\n",
       " 'any',\n",
       " 'brings',\n",
       " 'movie ever seen',\n",
       " 'worst thing',\n",
       " 'walked out',\n",
       " '30',\n",
       " 'real',\n",
       " 'worst films ever',\n",
       " 'waste of time money',\n",
       " 'great',\n",
       " 'one of worst movies',\n",
       " 'waste time or',\n",
       " 'as bad as',\n",
       " 'life of',\n",
       " 'wasted time',\n",
       " 'had high',\n",
       " 'gave 2',\n",
       " 'best movies',\n",
       " 'story told',\n",
       " 'insult to',\n",
       " 'this thing',\n",
       " 'fell in love with',\n",
       " 'heart warming',\n",
       " 'would love to',\n",
       " '10 out of',\n",
       " 'good idea',\n",
       " 'nothing new',\n",
       " 'in love',\n",
       " 'bad acting bad',\n",
       " 'magnificent',\n",
       " 'badly made',\n",
       " 'this junk',\n",
       " 'from outer',\n",
       " 'just',\n",
       " 'not save',\n",
       " 'lazy',\n",
       " 'not funny at all',\n",
       " 'story about',\n",
       " 'dreams',\n",
       " 'yet',\n",
       " 'camera',\n",
       " 'bad this movie',\n",
       " 'explanation',\n",
       " 'bottom of',\n",
       " 'animation',\n",
       " 'really really bad',\n",
       " 'holes',\n",
       " 'ludicrous',\n",
       " 'to be funny',\n",
       " 'very',\n",
       " 'very good',\n",
       " 'enjoyable',\n",
       " 'adds to',\n",
       " 'sub par',\n",
       " 'boring',\n",
       " 'no suspense',\n",
       " 'devotion',\n",
       " 'even remotely',\n",
       " 'fake',\n",
       " 'pleasantly',\n",
       " 'only saving grace',\n",
       " 'growing',\n",
       " 'later',\n",
       " 'reason to',\n",
       " 'fails',\n",
       " 'creature',\n",
       " 'as good as',\n",
       " 'did not make',\n",
       " 'also good',\n",
       " 'there not',\n",
       " 'worst movie',\n",
       " 'dvd',\n",
       " '90 minutes',\n",
       " 'emotions',\n",
       " 'stupid',\n",
       " 'nowhere',\n",
       " 'steals',\n",
       " 'suppose',\n",
       " 'growing up',\n",
       " 'seconds',\n",
       " 'movie waste',\n",
       " 'performances by',\n",
       " '1 out',\n",
       " 'strength',\n",
       " 'trash',\n",
       " 'does great',\n",
       " 'not bother with',\n",
       " 'sadness',\n",
       " 'fabulous',\n",
       " 'if going to',\n",
       " 'confusing',\n",
       " 'courage',\n",
       " 'tries',\n",
       " 'complex',\n",
       " 'acting horrible',\n",
       " 'lifeless',\n",
       " 'not enough',\n",
       " 'do not waste money',\n",
       " 'liked this',\n",
       " 'stick',\n",
       " 'mess',\n",
       " 'day',\n",
       " '7 10',\n",
       " 'cash in',\n",
       " 'success',\n",
       " 'great performances',\n",
       " 'turd',\n",
       " 'jean',\n",
       " 'great job',\n",
       " '9 out of',\n",
       " 'there nothing',\n",
       " 'not even close',\n",
       " 'does not make',\n",
       " 'power of',\n",
       " 'lack of',\n",
       " 'like bad',\n",
       " 'same time',\n",
       " 'rented this',\n",
       " 'so called',\n",
       " 'of love',\n",
       " 'fast forward',\n",
       " 'makes no sense',\n",
       " 'nicely',\n",
       " 'wanted to like',\n",
       " 'romance',\n",
       " 'not bother',\n",
       " 'moving',\n",
       " 'time with this',\n",
       " 'first saw this movie',\n",
       " 'attempt',\n",
       " 'inane',\n",
       " 'themes',\n",
       " 'surprising',\n",
       " 'shelf',\n",
       " 'first saw',\n",
       " 'horrible movie',\n",
       " 'come up',\n",
       " 'supposed to be',\n",
       " 'stunk',\n",
       " 'loving',\n",
       " 'editing',\n",
       " 'completely',\n",
       " 'bland',\n",
       " 'does not even',\n",
       " 'junk',\n",
       " 'nor',\n",
       " 'stupidest',\n",
       " 'at times',\n",
       " 'bad actors',\n",
       " 'even better',\n",
       " 'follows',\n",
       " 'science theater 3000',\n",
       " 'comedies',\n",
       " 'idiots',\n",
       " 'abysmal',\n",
       " 'intense',\n",
       " 'nothing but',\n",
       " 'matthau',\n",
       " 'to find',\n",
       " 'sense at all',\n",
       " 'fails to',\n",
       " 'does not help',\n",
       " 'be missed',\n",
       " 'do yourself',\n",
       " 'fell in love',\n",
       " 'low',\n",
       " 'charming',\n",
       " 'new',\n",
       " 'seemed',\n",
       " 'some',\n",
       " 'avoid',\n",
       " 'bother with this',\n",
       " 'steaming',\n",
       " 'loved',\n",
       " 'neither',\n",
       " 'unintentionally',\n",
       " 'so stupid',\n",
       " 'even close to',\n",
       " 'this just',\n",
       " 'wonderful',\n",
       " 'war',\n",
       " 'not miss',\n",
       " 'horror',\n",
       " 'sorry',\n",
       " 'released',\n",
       " 'attempts at',\n",
       " 'not very good',\n",
       " 'this pile',\n",
       " 'what point',\n",
       " 'never get back',\n",
       " 'of worst movies ever',\n",
       " 'stupid movie',\n",
       " 'plain bad',\n",
       " 'flawless',\n",
       " 'shows how',\n",
       " 'movie not worth',\n",
       " 'recently',\n",
       " 'half way',\n",
       " 'perfect',\n",
       " 'sounds like',\n",
       " 'best of all',\n",
       " 'watching this movie',\n",
       " 'one of greatest',\n",
       " 'refreshing',\n",
       " 'guess',\n",
       " 'job of',\n",
       " 'ripped',\n",
       " 'rent',\n",
       " 'touched',\n",
       " 'entire movie',\n",
       " 'best film',\n",
       " 'to cash in',\n",
       " 'piece of junk',\n",
       " 'of seat',\n",
       " 'this movie sucks',\n",
       " 'idiotic',\n",
       " 'unfortunately this',\n",
       " 'movie terrible',\n",
       " 'alright',\n",
       " 'to perfection',\n",
       " 'minutes of life',\n",
       " 'dumb',\n",
       " 'am',\n",
       " 'could',\n",
       " 'poorly written',\n",
       " 'witty',\n",
       " 'movie 2',\n",
       " 'very funny',\n",
       " 'great to',\n",
       " 'do not waste time',\n",
       " 'could not',\n",
       " 'quiet',\n",
       " 'plot holes',\n",
       " 'viewing',\n",
       " 'otherwise',\n",
       " 'waste time or money',\n",
       " 'embarrassment',\n",
       " 'very little',\n",
       " 'this must see',\n",
       " 'did not care',\n",
       " '7',\n",
       " 'thinking',\n",
       " 'of original',\n",
       " 'any real',\n",
       " 'ed wood',\n",
       " 'extremely well',\n",
       " 'even funny',\n",
       " 'this drivel',\n",
       " 'try',\n",
       " 'loneliness',\n",
       " 'acting bad',\n",
       " 'seems',\n",
       " 'ages',\n",
       " 'what makes',\n",
       " 'idea',\n",
       " 'to be worst',\n",
       " '1 out of',\n",
       " 'rip',\n",
       " 'familiar',\n",
       " 'acting terrible',\n",
       " 'bottom',\n",
       " 'there absolutely no',\n",
       " 'well worth',\n",
       " 'first off',\n",
       " 'relationship',\n",
       " 'both',\n",
       " '9 10',\n",
       " 'walked',\n",
       " 'delivers',\n",
       " 'running',\n",
       " 'even worse than',\n",
       " 'insipid',\n",
       " 'even',\n",
       " 'this lame',\n",
       " 'of finest',\n",
       " 'insulting',\n",
       " 'amazing',\n",
       " 'recommend this',\n",
       " 'continuity',\n",
       " 'warned',\n",
       " 'worst movie ever seen',\n",
       " 'train wreck',\n",
       " 'music',\n",
       " 'spent',\n",
       " 'wasting',\n",
       " 'could made',\n",
       " 'superbly',\n",
       " 'different',\n",
       " 'this great',\n",
       " 'start with',\n",
       " 'must see for',\n",
       " 'just not',\n",
       " 'of best ever',\n",
       " 'bad films',\n",
       " 'wonderful film',\n",
       " 'of best movies',\n",
       " 'innocent',\n",
       " 'would not recommend this',\n",
       " 'contrast',\n",
       " 'waste money on',\n",
       " 'unfortunately',\n",
       " 'may not',\n",
       " 'shows',\n",
       " 'job',\n",
       " 'stay away from this',\n",
       " 'some great',\n",
       " 'nothing more than',\n",
       " 'journey',\n",
       " 'scary',\n",
       " 'one of favorites',\n",
       " 'in love with',\n",
       " 'fresh',\n",
       " 'as worst',\n",
       " 'if want to see',\n",
       " 'most boring',\n",
       " 'society',\n",
       " 'accent',\n",
       " 'great story',\n",
       " '10 out',\n",
       " 'view',\n",
       " 'one of finest',\n",
       " 'debut',\n",
       " 'ugly',\n",
       " 'so awful',\n",
       " 'trying to be',\n",
       " 'loved this movie',\n",
       " 'rip off',\n",
       " 'annoying',\n",
       " 'not make sense',\n",
       " 'unforgettable',\n",
       " 'irritating',\n",
       " 'role of',\n",
       " 'laughably',\n",
       " 'stock',\n",
       " 'soundtrack',\n",
       " 'first time',\n",
       " 'delightful',\n",
       " 'absolutely no',\n",
       " 'on plus side',\n",
       " 'supposed to',\n",
       " 'early',\n",
       " '9 out of 10',\n",
       " 'surreal',\n",
       " 'still',\n",
       " 'wonder',\n",
       " 'for no apparent reason',\n",
       " 'fantastic',\n",
       " 'cheap',\n",
       " 'how awful',\n",
       " 'brutal',\n",
       " 'this worst movie',\n",
       " 'coherent',\n",
       " 'manos',\n",
       " 'been better',\n",
       " 'movie could',\n",
       " 'thoroughly enjoyed',\n",
       " 'marriage',\n",
       " 'wanted to like this',\n",
       " 'look like',\n",
       " 'promising',\n",
       " 'money on',\n",
       " 'excellent as',\n",
       " '30 minutes',\n",
       " 'avoid at',\n",
       " 'just did',\n",
       " 'stupidity',\n",
       " 'of trash',\n",
       " 'thrown',\n",
       " 'in opinion',\n",
       " 'terrible',\n",
       " 'script',\n",
       " 'made no',\n",
       " 'say about this',\n",
       " 'recommended',\n",
       " 'start',\n",
       " 'way too',\n",
       " 'this piece of crap',\n",
       " 'one dimensional',\n",
       " 'of family',\n",
       " 'bothered to',\n",
       " 'genius',\n",
       " 'away from this',\n",
       " 'this stinker',\n",
       " 'failure',\n",
       " 'worst ever',\n",
       " '1',\n",
       " 'consists',\n",
       " 'do not believe',\n",
       " 'not even funny',\n",
       " 'bunch',\n",
       " 'piece of garbage',\n",
       " 'of great',\n",
       " 'ok',\n",
       " 'why does',\n",
       " 'masterful',\n",
       " 'dialog',\n",
       " 'walter matthau',\n",
       " 'great to see',\n",
       " 'sense at',\n",
       " 'mystery science theater 3000',\n",
       " 'to do',\n",
       " 'fascinating',\n",
       " 'embarrassing',\n",
       " 'trying to',\n",
       " 'obviously',\n",
       " 'am not',\n",
       " 'awful this',\n",
       " 'film just',\n",
       " 'do not bother',\n",
       " 'some kind of',\n",
       " 'one of most',\n",
       " 'off',\n",
       " 'surprisingly',\n",
       " 'trite',\n",
       " 'rip off of',\n",
       " 'highly recommend',\n",
       " 'total waste',\n",
       " 'trying',\n",
       " 'to make movie',\n",
       " 'enjoy',\n",
       " 'really bad',\n",
       " 'to see again',\n",
       " 'if',\n",
       " 'young',\n",
       " 'well as',\n",
       " 'budget',\n",
       " 'wondering',\n",
       " 'impressed',\n",
       " 'era',\n",
       " 'where to begin',\n",
       " 'am sorry but',\n",
       " 'not good',\n",
       " 'at first',\n",
       " 'grade',\n",
       " 'painful to',\n",
       " 'of fun',\n",
       " 'supporting',\n",
       " 'do not see this',\n",
       " 'overlooked',\n",
       " 'ripped off',\n",
       " 'god',\n",
       " 'but unfortunately',\n",
       " 'would be',\n",
       " 'appalling',\n",
       " 'how not to make',\n",
       " 'on edge of seat',\n",
       " 'james stewart',\n",
       " 'crafted',\n",
       " 'uplifting',\n",
       " 'this excellent',\n",
       " 'b',\n",
       " 'genuine',\n",
       " 'no plot',\n",
       " 'viewings',\n",
       " 'bad as',\n",
       " 'care',\n",
       " 'seagal',\n",
       " 'but this movie',\n",
       " 'could not even',\n",
       " 'performances',\n",
       " 'want to see',\n",
       " 'total waste of',\n",
       " 'sequel',\n",
       " 'allows',\n",
       " 'james',\n",
       " 'that about',\n",
       " 'pile',\n",
       " 'come up with',\n",
       " 'excellent movie',\n",
       " 'laugh',\n",
       " 'complete waste of',\n",
       " 'liked',\n",
       " 'brought to',\n",
       " 'best performances',\n",
       " 'ways',\n",
       " 'enjoyed this movie',\n",
       " 'let',\n",
       " '2 out of',\n",
       " 'no story',\n",
       " 'hooked',\n",
       " 'performances as',\n",
       " 'been good',\n",
       " 'brilliant as',\n",
       " 'blame',\n",
       " 'all time',\n",
       " 'porno',\n",
       " 'barely',\n",
       " 'this movie',\n",
       " 'what waste',\n",
       " 'poor acting',\n",
       " 'how not',\n",
       " '8 out of',\n",
       " 'so bad',\n",
       " 'downhill',\n",
       " 'happy',\n",
       " 'one of favorite',\n",
       " 'will not regret',\n",
       " 'unfolds',\n",
       " 'happiness',\n",
       " 'boring film',\n",
       " 'all costs',\n",
       " 'as well as',\n",
       " 'say about',\n",
       " 'bad movie',\n",
       " 'straight',\n",
       " 'music by',\n",
       " 'not one',\n",
       " 'story',\n",
       " 'plan 9',\n",
       " 'going',\n",
       " 'good thing about',\n",
       " 'years later',\n",
       " 'ok but',\n",
       " 'this awful',\n",
       " '3 out',\n",
       " 'to do with',\n",
       " 'bad bad',\n",
       " 'this supposed',\n",
       " 'piece of',\n",
       " 'story of',\n",
       " 'little film',\n",
       " 'definitely worth',\n",
       " 'this great film',\n",
       " 'not waste time on',\n",
       " 'tremendous',\n",
       " 'some sort of',\n",
       " 'bad in',\n",
       " 'see',\n",
       " 'performances from',\n",
       " 'does not',\n",
       " 'no idea',\n",
       " 'so poorly',\n",
       " 'seen worse',\n",
       " 'saying',\n",
       " 'say',\n",
       " 'of most',\n",
       " 'below average',\n",
       " 'performance of',\n",
       " 'going to',\n",
       " 'acting awful',\n",
       " 'science theater',\n",
       " 'ensemble',\n",
       " 'for this',\n",
       " 'thought provoking',\n",
       " 'atmosphere of',\n",
       " 'there no real',\n",
       " 'really really',\n",
       " 'gripping',\n",
       " 'be funny',\n",
       " 'highly recommend this movie',\n",
       " 'do not',\n",
       " 'another great',\n",
       " 'fast paced',\n",
       " 'painfully',\n",
       " 'actors',\n",
       " 'available on',\n",
       " ...]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features = list(set(top_features.tolist() + only_positive.tolist() + only_negative.tolist()))\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c40ab8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc624d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5e6b263",
   "metadata": {},
   "source": [
    "### Find best ridge-regression model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "321d893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the full training set using the top 2000 features\n",
    "\n",
    "top_feature_vectorizer = CountVectorizer(\n",
    "    vocabulary=top_features,          # The top 200 features\n",
    "    stop_words=stopwords,             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                     # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r\"\\b[\\w+\\|']+\\b\"    # Use word tokenizer, but don't split on apostrophes\n",
    ")\n",
    "\n",
    "dtm_vocab_train = top_feature_vectorizer.fit_transform(all_train[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddbceee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a403fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f577ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = LogisticRegressionCV(Cs=10, cv=5, penalty=\"l2\", scoring=\"roc_auc\", max_iter=100000, random_state=SEED, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "51d55b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_y = all_train[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e08870de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegressionCV(cv=5, max_iter=100000, random_state=4031,\n",
       "                     scoring=&#x27;roc_auc&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegressionCV</label><div class=\"sk-toggleable__content\"><pre>LogisticRegressionCV(cv=5, max_iter=100000, random_state=4031,\n",
       "                     scoring=&#x27;roc_auc&#x27;, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegressionCV(cv=5, max_iter=100000, random_state=4031,\n",
       "                     scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(dtm_vocab_train, all_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4c43380e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166.81005372000558"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_C = grid_search.C_[0]\n",
    "best_C   # best_C is 166.81005372"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4dbee9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([[0.90935775, 0.94354946, 0.96294727, 0.97165258, 0.97480854,\n",
       "         0.97552131, 0.97560374, 0.97560812, 0.97560816, 0.97560816],\n",
       "        [0.91301104, 0.9465028 , 0.96414   , 0.97174612, 0.97448715,\n",
       "         0.97511537, 0.97520611, 0.9752191 , 0.97521909, 0.97521908],\n",
       "        [0.91137743, 0.94582756, 0.96427532, 0.97222808, 0.97494514,\n",
       "         0.97547124, 0.97554414, 0.97555846, 0.97555846, 0.97555859],\n",
       "        [0.91270893, 0.9457004 , 0.96344614, 0.97148555, 0.97442697,\n",
       "         0.97511359, 0.97520342, 0.97520149, 0.97519389, 0.97519399],\n",
       "        [0.91288587, 0.94584362, 0.96394406, 0.97208396, 0.97515133,\n",
       "         0.97586374, 0.97594976, 0.97592957, 0.97590986, 0.97590988]])}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1eb3f",
   "metadata": {},
   "source": [
    "### How well do the 2000-ish terms predict movie review sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "efbe74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "62b84121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of split 1: 0.9562021316566655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of split 2: 0.9564398398628675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of split 3: 0.9546663902205452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of split 4: 0.9565502633921685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of split 5: 0.9548797058844237\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "\n",
    "for i in range(len(train_datasets)):\n",
    "    model = LogisticRegression(C=best_C, penalty=\"l2\", max_iter=100000, random_state=SEED, verbose=1)\n",
    "    \n",
    "    train_X = top_feature_vectorizer.fit_transform(preprocess_reviews(train_datasets[i][\"review\"]))\n",
    "    train_y = train_datasets[i][\"sentiment\"]\n",
    "    \n",
    "    model.fit(train_X, train_y)\n",
    "    \n",
    "    test_X = top_feature_vectorizer.transform(preprocess_reviews(test_datasets[i][\"review\"]))\n",
    "    test_y = test_ys[i][\"sentiment\"]\n",
    "    \n",
    "    pred_y = model.predict_proba(test_X)[:, 1]  # Predict probabilities for class 1 (positive review)\n",
    "    \n",
    "    auc_score = roc_auc_score(test_y, pred_y)\n",
    "    auc_scores.append(auc_score)\n",
    "    \n",
    "    print(f\"AUC of split {i+1}: {auc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd483d2",
   "metadata": {},
   "source": [
    "### Reduce the number of terms to under 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e1c9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df2100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1612471",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Ignore code below this line</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5e0c0",
   "metadata": {},
   "source": [
    "## Attempt at lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add lemmatizer to preprocessing.\n",
    "# Code taken from StackOverflow post: https://stackoverflow.com/questions/47423854/sklearn-adding-lemmatizer-to-countvectorizer\n",
    "\n",
    "# To make this work, need to deal with punctuation. Also need to provide POS tags, like here:\n",
    "# https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68aa2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_vectorizer = CountVectorizer(\n",
    "    lowercase=True,                   # Convert to lowercase\n",
    "    tokenizer=LemmaTokenizer(),       # Lemmatization\n",
    "    stop_words=stopwords,             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                     # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r\"\\b[\\w+\\|']+\\b\"    # Use word tokenizer\n",
    ")\n",
    "\n",
    "lemma_dtm_train = lemma_vectorizer.fit_transform(train['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000fccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some of the ngrams identified\n",
    "lemma_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7335d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the number of ngrams\n",
    "lemma_vectorizer.get_feature_names_out().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2764e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode as Unicode\n",
    "feature_names = [l.encode(\"utf-8\") for l in lemma_vectorizer.get_feature_names_out()]\n",
    "\n",
    "# Output features to file\n",
    "with open(\"split_1/lemma_vectorizer_features.txt\", \"wb\") as f:\n",
    "    for l in feature_names:\n",
    "        f.write(b'%s\\n'%l)\n",
    "\n",
    "np.savetxt(\"split_1/lemma_vectorizer_features.txt\", lemma_vectorizer.get_feature_names_out(), fmt=\"%s\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d680c",
   "metadata": {},
   "source": [
    "## Try stemming words before computing their predictive power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0879f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stem = train_datasets[3]\n",
    "# Remove HTML tags\n",
    "train_stem[\"review\"] = train_stem[\"review\"].str.replace('<.*?>', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd647c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efef427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctulation besides apostrophes.\n",
    "# Regular expression from StackOverflow post:\n",
    "# https://stackoverflow.com/questions/59877761/how-to-strip-string-from-punctuation-except-apostrophes-for-nlp\n",
    "train_stem[\"review\"] = train_stem[\"review\"].str.replace('[^\\w\\d\\s\\']+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02962f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try whitespace tokenizer to avoid splitting words on apostrophes\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tk = WhitespaceTokenizer()\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf7960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk import word_tokenize\n",
    "\n",
    "for i in range(len(train_stem)):\n",
    "    train_stem.iloc[i, 2] = \" \".join([p_stemmer.stem(review_token) for review_token in tk.tokenize(train_stem.iloc[i, 2])\n",
    "                                      if review_token not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5a4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaad5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorterStemmer class to add to preprocessing.\n",
    "class StemmerPorter(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = PorterStemmer()\n",
    "        self.tk = WhitespaceTokenizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.stem(t) for t in self.tk.tokenize(articles) if t not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a3a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_vectorizer = CountVectorizer(\n",
    "    lowercase=True,                   # Convert to lowercase\n",
    "    tokenizer=StemmerPorter(),        # Stemming\n",
    "    stop_words=stopwords,             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                     # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r'[^\\w\\d\\s\\']+'     # Keep apostrophes while removing other punctuation\n",
    ")\n",
    "\n",
    "dtm_train_stem = stem_vectorizer.fit_transform(train_stem['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9225536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some of the ngrams identified\n",
    "stem_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e965bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode as Unicode\n",
    "stem_feature_names = [l.encode(\"utf-8\") for l in stem_vectorizer.get_feature_names_out()]\n",
    "\n",
    "# Output features to file\n",
    "with open(\"split_1/stem_vectorizer_features.txt\", \"wb\") as f:\n",
    "    for l in stem_feature_names:\n",
    "        f.write(b'%s\\n'%l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try t-test to identify terms that are strongly associated with only positive or only negative reviews.\n",
    "\n",
    "dtm_stem_array = dtm_train_stem.toarray()\n",
    "dtm_stem_pos = dtm_stem_array[train_stem.sentiment == 1, :]\n",
    "dtm_stem_neg = dtm_stem_array[train_stem.sentiment == 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e54411",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_stem_pos_count = dtm_stem_pos.shape[0]\n",
    "dtm_stem_neg_count = dtm_stem_neg.shape[0]\n",
    "dtm_stem_pos_count, dtm_stem_neg_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_stem_pos_means = np.mean(dtm_stem_pos, axis=0)\n",
    "dtm_stem_pos_vars = np.var(dtm_stem_pos, axis=0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_stem_neg_means = np.mean(dtm_stem_neg, axis=0)\n",
    "dtm_stem_neg_vars = np.var(dtm_stem_neg, axis=0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8998f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each term / ngram, compute t-statistic for two independent samples.\n",
    "# Hmmm...they're not independent, but we can't really pool the variance...\n",
    "\n",
    "stem_t_statistics = (dtm_stem_pos_means - dtm_stem_neg_means) / np.sqrt((dtm_stem_pos_vars/dtm_stem_pos_count) + (dtm_stem_neg_vars/dtm_stem_neg_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb452544",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_feature_ngrams = stem_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19296a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_feature_statistic_df = pd.DataFrame({\"feature\": stem_feature_ngrams.tolist(), \"statistic\": stem_t_statistics.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many terms meet the 0.05 significance threshold?\n",
    "\n",
    "len(feature_statistic_df[abs(feature_statistic_df.statistic) >= 1.645])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d96629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at top 50 positive ngrams\n",
    "stem_feature_statistic_df.sort_values(by=\"statistic\", ascending=False).iloc[0:50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e602aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at bottom 50 ngrams (most negative)\n",
    "stem_feature_statistic_df.sort_values(by=\"statistic\").iloc[0:50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a20bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the 2000 most predictive tokens based on their t-statistics\n",
    "n_tokens = 2000\n",
    "\n",
    "stem_feature_statistic_df[\"abs_statistic\"] = abs(stem_feature_statistic_df[\"statistic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b94fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_predictive_tokens = stem_feature_statistic_df.sort_values(by=\"abs_statistic\", ascending=False).iloc[:n_tokens, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34151247",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_predictive_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add words that only appeared in positive reviews\n",
    "dtm_stem_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_feature_ngrams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_positive = stem_feature_ngrams[np.logical_and((dtm_stem_pos_means > 0), (dtm_stem_neg_means == 0))]\n",
    "only_negative = stem_feature_ngrams[np.logical_and((dtm_stem_pos_means == 0), (dtm_stem_neg_means > 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49629e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a3a61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
