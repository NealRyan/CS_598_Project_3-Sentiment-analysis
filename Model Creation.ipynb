{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Set seed\n",
    "SEED = 4031\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file locations and names\n",
    "\n",
    "project_root_dir = \"Data\"\n",
    "project_subdir_prefix = \"split_\"\n",
    "train_data_filename = \"train.tsv\"\n",
    "test_data_filename = \"test.tsv\"\n",
    "test_y_data_filename = \"test_y.tsv\"\n",
    "\n",
    "\n",
    "# The number of train/test data folders and the target RMSE for each\n",
    "# train/test split in each folder\n",
    "\n",
    "n_datasets = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of data subfolders, each with a separate training and test set.\n",
    "\n",
    "os_walk = os.walk(project_root_dir)\n",
    "data_subdir_list = [subdirs for root, subdirs, files in os_walk][0]\n",
    "n_subdirs = len(data_subdir_list)\n",
    "\n",
    "assert(n_subdirs == n_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for training and test datasets\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "test_y_datasets = []\n",
    "\n",
    "\n",
    "# Loop over subfolders and read in training/test datasets and test weekly sales.\n",
    "# Use a loop instead of using os.walk directly to avoid \"fold10\" immediately following \"fold1\".\n",
    "\n",
    "for subdir_num in np.arange(n_subdirs) + 1:\n",
    "    subdir_num_str = str(subdir_num)\n",
    "    train_datasets.append(pd.read_csv(os.path.join(project_root_dir,\n",
    "                                                   project_subdir_prefix + subdir_num_str,\n",
    "                                                   train_data_filename), sep='\\t', header=0, dtype=str))\n",
    "    test_datasets.append(pd.read_csv(os.path.join(project_root_dir,\n",
    "                                                   project_subdir_prefix + subdir_num_str,\n",
    "                                                   test_data_filename), sep='\\t', header=0, dtype=str))\n",
    "    test_y_datasets.append(pd.read_csv(os.path.join(project_root_dir,\n",
    "                                                   project_subdir_prefix + subdir_num_str,\n",
    "                                                   test_y_data_filename), sep='\\t', header=0, dtype=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_datasets)):\n",
    "    train_datasets[i]['review'] = train_datasets[i]['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)\n",
    "    test_datasets[i]['review'] = test_datasets[i]['review'].str.replace('&lt;.*?&gt;', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokeninzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pool all datasets into one large set\n",
    "all_words = []\n",
    "for i in range(len(train_datasets)):\n",
    "    for j in range(len(train_datasets[i]['review'])):\n",
    "        all_words.append(train_datasets[i]['review'][j])\n",
    "    for j in range(len(test_datasets[i]['review'])):\n",
    "        all_words.append(test_datasets[i]['review'][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = [\"i\", \"me\", \"my\", \"myself\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    preprocessor=lambda x: x.lower(),  # Convert to lowercase\n",
    "    stop_words='english',             # Remove stop words\n",
    "    ngram_range=(1, 4),               # Use 1- to 4-grams\n",
    "    min_df=0.001,                        # Minimum term frequency\n",
    "    max_df=0.5,                       # Maximum document frequency\n",
    "    token_pattern=r'\\b\\w+\\b'          # Use word tokenizer\n",
    ")\n",
    "\n",
    "dtm_train = vectorizer.fit_transform(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<250000x16719 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 24022745 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Lasso Regression to find the most predicitive tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare all training data into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train = pd.DataFrame()\n",
    "\n",
    "for train_df in train_datasets:\n",
    "    all_train = pd.concat([all_train, train_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_tokens(num, c):\n",
    "    for i in range(10):\n",
    "        lasso_log_model = LogisticRegression(C=c, penalty='l1', solver='liblinear', max_iter=100000)  # very high max iter to ensure converge\n",
    "        X_train = vectorizer.transform(all_train['review'])\n",
    "        Y_train = all_train['sentiment']\n",
    "        lasso_log_model.fit(X_train, Y_train)\n",
    "\n",
    "        best_tokens = [[i, coef] for i, coef in enumerate(lasso_log_model.coef_[0]) if coef != 0]\n",
    "\n",
    "        num_tokens = len(best_tokens)\n",
    "        print(f'number of tokens: {num_tokens}')\n",
    "        print(f'old c: {c}')\n",
    "\n",
    "        if num_tokens in range(num, num+100):\n",
    "            return best_tokens\n",
    "        elif num_tokens > num+100:\n",
    "            c = c-(num_tokens/num*0.1*c)\n",
    "        elif num_tokens < num:\n",
    "            c = c+(num/num_tokens*0.1*c)\n",
    "\n",
    "        print(f'new c: {c}')\n",
    "    print(\"Bad initial c value, try another value\")\n",
    "    raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519\n",
      "0.0325\n",
      "0.03677913100724161\n",
      "1696\n",
      "0.03677913100724161\n",
      "0.04111629268262387\n",
      "1869\n",
      "0.04111629268262387\n",
      "0.04551610998413526\n",
      "2049\n",
      "0.04551610998413526\n",
      "Total tokens after regularization: 2049\n"
     ]
    }
   ],
   "source": [
    "#Find all tokens with non-zero coefficients after lasso\n",
    "best_tokens = find_best_tokens(num=2000, c=0.0455)\n",
    "print(f'Total tokens after regularization: {len(best_tokens)}')\n",
    "\n",
    "copy1_best_tokens = best_tokens[:]\n",
    "copy2_best_tokens = best_tokens[:]\n",
    "\n",
    "#Sort tokens by coefficients, more extreme = more predictive\n",
    "new_best_tokens = list(map(lambda x: [x[0], x[1]], copy1_best_tokens))\n",
    "sorted_best_tokens = sorted(new_best_tokens, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "abs_best_tokens = list(map(lambda x: [x[0], abs(x[1])], copy2_best_tokens))\n",
    "sorted_abs_best_tokens = sorted(abs_best_tokens, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Take the top 2000 tokens regardless of pos/neg\n",
    "top_2000_tokens = sorted_abs_best_tokens[:2000]\n",
    "\n",
    "#Take the top 50 from pos and neg for explainability\n",
    "positive_tokens = sorted_best_tokens[0:50]\n",
    "negative_tokens = sorted_best_tokens[-51:-1]\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "#Make tokens for model\n",
    "model_tokens = []\n",
    "\n",
    "for token in top_2000_tokens:\n",
    "    model_tokens.append(feature_names[token[0]])\n",
    "\n",
    "\n",
    "positive_predictors = []\n",
    "negative_predictors = []\n",
    "#Make tokens for explainability:\n",
    "for (pos, neg) in zip(positive_tokens, negative_tokens):\n",
    "    positive_predictors.append(feature_names[pos[0]])\n",
    "    negative_predictors.append(feature_names[neg[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Customer Tokenizer from only the 2000 best tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if token in model_tokens]\n",
    "    return filtered_tokens\n",
    "\n",
    "custom_vectorizer = CountVectorizer(tokenizer=custom_tokenizer, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vectorizer = CountVectorizer(binary=True, vocabulary=model_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "aucs = []\n",
    "\n",
    "for i in range(len(train_datasets)):\n",
    "    model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "    X_train = custom_vectorizer.fit_transform(train_datasets[i]['review'])\n",
    "    Y_train = train_datasets[i]['sentiment']\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    X_test = custom_vectorizer.fit_transform(test_datasets[i]['review'])\n",
    "    Y_test = test_y_datasets[i]['sentiment']\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    auc = roc_auc_score(Y_test, Y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    aucs.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of fold 1: 0.8849712137216947\n",
      "AUC of fold 2: 0.8819008615866393\n",
      "AUC of fold 3: 0.8840851923729133\n",
      "AUC of fold 4: 0.8849234143509852\n",
      "AUC of fold 5: 0.8812036964142502\n"
     ]
    }
   ],
   "source": [
    "for i, (accuracy, auc) in enumerate(zip(accuracies, aucs)):\n",
    "    #print(f\"Accuracy of fold {i+1}: {accuracy}\")\n",
    "    print(f\"AUC of fold {i+1}: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Tokens for Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most predictive positive token #1: 7 10\n",
      "Most predictive positive token #2: 8 10\n",
      "Most predictive positive token #3: 10 10\n",
      "Most predictive positive token #4: refreshing\n",
      "Most predictive positive token #5: t disappointed\n",
      "Most predictive positive token #6: 9 10\n",
      "Most predictive positive token #7: superb\n",
      "Most predictive positive token #8: excellent\n",
      "Most predictive positive token #9: wonderfully\n",
      "Most predictive positive token #10: funniest\n",
      "Most predictive positive token #11: brilliantly\n",
      "Most predictive positive token #12: gem\n",
      "Most predictive positive token #13: amazing\n",
      "Most predictive positive token #14: finest\n",
      "Most predictive positive token #15: outstanding\n",
      "Most predictive positive token #16: definitely worth\n",
      "Most predictive positive token #17: hilarious\n",
      "Most predictive positive token #18: pleasantly surprised\n",
      "Most predictive positive token #19: highly recommended\n",
      "Most predictive positive token #20: subtle\n",
      "Most predictive positive token #21: fantastic\n",
      "Most predictive positive token #22: incredible\n",
      "Most predictive positive token #23: highly recommend\n",
      "Most predictive positive token #24: wonderful\n",
      "Most predictive positive token #25: solid\n",
      "Most predictive positive token #26: perfect\n",
      "Most predictive positive token #27: love movie\n",
      "Most predictive positive token #28: 7\n",
      "Most predictive positive token #29: brilliant\n",
      "Most predictive positive token #30: bad thing\n",
      "Most predictive positive token #31: underrated\n",
      "Most predictive positive token #32: favorite\n",
      "Most predictive positive token #33: perfectly\n",
      "Most predictive positive token #34: rare\n",
      "Most predictive positive token #35: loved movie\n",
      "Most predictive positive token #36: beautifully\n",
      "Most predictive positive token #37: pay attention\n",
      "Most predictive positive token #38: hooked\n",
      "Most predictive positive token #39: extraordinary\n",
      "Most predictive positive token #40: surprisingly\n",
      "Most predictive positive token #41: powerful\n",
      "Most predictive positive token #42: impressed\n",
      "Most predictive positive token #43: enjoyable\n",
      "Most predictive positive token #44: surprised\n",
      "Most predictive positive token #45: awesome\n",
      "Most predictive positive token #46: noir\n",
      "Most predictive positive token #47: 8\n",
      "Most predictive positive token #48: delightful\n",
      "Most predictive positive token #49: loved\n",
      "Most predictive positive token #50: favourite\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(positive_predictors):\n",
    "    print(f'Most predictive positive token #{i+1}: {token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most predictive negative token #1: annoying\n",
      "Most predictive negative token #2: badly\n",
      "Most predictive negative token #3: ridiculous\n",
      "Most predictive negative token #4: hoping\n",
      "Most predictive negative token #5: t recommend\n",
      "Most predictive negative token #6: t funny\n",
      "Most predictive negative token #7: wooden\n",
      "Most predictive negative token #8: bland\n",
      "Most predictive negative token #9: wasted\n",
      "Most predictive negative token #10: trite\n",
      "Most predictive negative token #11: worse\n",
      "Most predictive negative token #12: mess\n",
      "Most predictive negative token #13: horrible\n",
      "Most predictive negative token #14: boring\n",
      "Most predictive negative token #15: stinker\n",
      "Most predictive negative token #16: sorry\n",
      "Most predictive negative token #17: lame\n",
      "Most predictive negative token #18: pathetic\n",
      "Most predictive negative token #19: lousy\n",
      "Most predictive negative token #20: pretentious\n",
      "Most predictive negative token #21: pointless\n",
      "Most predictive negative token #22: mildly\n",
      "Most predictive negative token #23: terrible\n",
      "Most predictive negative token #24: dreadful\n",
      "Most predictive negative token #25: miscast\n",
      "Most predictive negative token #26: appalling\n",
      "Most predictive negative token #27: uninteresting\n",
      "Most predictive negative token #28: avoid\n",
      "Most predictive negative token #29: olds\n",
      "Most predictive negative token #30: disappointing\n",
      "Most predictive negative token #31: incoherent\n",
      "Most predictive negative token #32: fails\n",
      "Most predictive negative token #33: mediocre\n",
      "Most predictive negative token #34: lacks\n",
      "Most predictive negative token #35: wasting\n",
      "Most predictive negative token #36: dull\n",
      "Most predictive negative token #37: tedious\n",
      "Most predictive negative token #38: laughable\n",
      "Most predictive negative token #39: unfunny\n",
      "Most predictive negative token #40: redeeming\n",
      "Most predictive negative token #41: forgettable\n",
      "Most predictive negative token #42: poorly\n",
      "Most predictive negative token #43: awful\n",
      "Most predictive negative token #44: mst3k\n",
      "Most predictive negative token #45: disappointment\n",
      "Most predictive negative token #46: worst\n",
      "Most predictive negative token #47: 2 10\n",
      "Most predictive negative token #48: waste\n",
      "Most predictive negative token #49: 1 10\n",
      "Most predictive negative token #50: 4 10\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(negative_predictors):\n",
    "    print(f'Most predictive negative token #{i+1}: {token}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
